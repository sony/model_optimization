

<!doctype html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Keras Quantization Aware Training Model Init &#8212; MCT Documentation: ver 1.6.0</title>
    <link rel="stylesheet" type="text/css" href="../../../static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../static/bizstyle.css" />
    <link rel="stylesheet" type="text/css" href="../../../static/css/custom.css" />
    
    <script data-url_root="../../../" id="documentation_options" src="../../../static/documentation_options.js"></script>
    <script src="../../../static/jquery.js"></script>
    <script src="../../../static/underscore.js"></script>
    <script src="../../../static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../static/doctools.js"></script>
    <script src="../../../static/bizstyle.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0" />
    <!--[if lt IE 9]>
    <script src="static/css3-mediaqueries.js"></script>
    <![endif]-->
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">MCT Documentation: ver 1.6.0</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Keras Quantization Aware Training Model Init</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="keras-quantization-aware-training-model-init">
<span id="ug-keras-quantization-aware-training-init"></span><h1>Keras Quantization Aware Training Model Init<a class="headerlink" href="#keras-quantization-aware-training-model-init" title="Permalink to this heading">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="model_compression_toolkit.keras_quantization_aware_training_init">
<span class="sig-prename descclassname"><span class="pre">model_compression_toolkit.</span></span><span class="sig-name descname"><span class="pre">keras_quantization_aware_training_init</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">in_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">representative_data_gen</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_kpi</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">core_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">CoreConfig()</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fw_info</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">DEFAULT_KERAS_INFO</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_platform_capabilities</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">DEFAULT_KERAS_TPC</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#model_compression_toolkit.keras_quantization_aware_training_init" title="Permalink to this definition">¶</a></dt>
<dd><p>Prepare a trained Keras model for quantization aware training. First the model quantization is optimized
with post-training quantization, then the model layers are wrapped with QuantizeWrappers. The model is
quantized using a symmetric quantization thresholds (power of two).
The model is first optimized using several transformations (e.g. BatchNormalization folding to
preceding layers). Then, using a given dataset, statistics (e.g. min/max, histogram, etc.) are
being collected for each layer’s output (and input, depends on the quantization configuration).
For each possible bit width (per layer) a threshold is then being calculated using the collected
statistics. Then, if given a mixed precision config in the core_config, using an ILP solver we find
a mixed-precision configuration, and set a bit-width for each layer. The model is built with fake_quant
nodes for quantizing activation. Weights are kept as float and are quantized online while training by the
quantization wrapper’s weight quantizer.
In order to limit the maximal model’s size, a target KPI need to be passed after weights_memory
is set (in bytes).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>in_model</strong> (<em>Model</em>) – Keras model to quantize.</p></li>
<li><p><strong>representative_data_gen</strong> (<em>Callable</em>) – Dataset used for initial calibration.</p></li>
<li><p><strong>target_kpi</strong> (<a class="reference internal" href="../modules/mixed_precision_quantization_config.html#model_compression_toolkit.KPI" title="model_compression_toolkit.KPI"><em>KPI</em></a>) – KPI object to limit the search of the mixed-precision configuration as desired.</p></li>
<li><p><strong>core_config</strong> (<a class="reference internal" href="../modules/core_config.html#model_compression_toolkit.CoreConfig" title="model_compression_toolkit.CoreConfig"><em>CoreConfig</em></a>) – Configuration object containing parameters of how the model should be quantized, including mixed precision parameters.</p></li>
<li><p><strong>fw_info</strong> (<a class="reference internal" href="../classes/FrameworkInfo.html#model_compression_toolkit.FrameworkInfo" title="model_compression_toolkit.FrameworkInfo"><em>FrameworkInfo</em></a>) – Information needed for quantization about the specific framework (e.g., kernel channels indices, groups of layers by how they should be quantized, etc.).  <a class="reference external" href="https://github.com/sony/model_optimization/blob/main/model_compression_toolkit/core/keras/default_framework_info.py">Default Keras info</a></p></li>
<li><p><strong>target_platform_capabilities</strong> (<a class="reference internal" href="../modules/target_platform.html#model_compression_toolkit.target_platform.TargetPlatformCapabilities" title="model_compression_toolkit.target_platform.TargetPlatformCapabilities"><em>TargetPlatformCapabilities</em></a>) – TargetPlatformCapabilities to optimize the Keras model according to.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A quantized model.
User information that may be needed to handle the quantized model.
Custom-Objects dictionary for loading the saved kers model.</p>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Import MCT:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">model_compression_toolkit</span> <span class="k">as</span> <span class="nn">mct</span>
</pre></div>
</div>
<p>Import a Keras model:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tensorflow.keras.applications.mobilenet_v2</span> <span class="kn">import</span> <span class="n">MobileNetV2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">MobileNetV2</span><span class="p">()</span>
</pre></div>
</div>
<p>Create a random dataset generator:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">repr_datagen</span><span class="p">():</span> <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">224</span><span class="p">,</span><span class="mi">3</span><span class="p">))]</span>
</pre></div>
</div>
<p>Create a MCT core config, containing the quantization configuration:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">mct</span><span class="o">.</span><span class="n">CoreConfig</span><span class="p">()</span>
</pre></div>
</div>
<p>If mixed precision is desired, create a MCT core config with a mixed-precision configuration, to quantize a model with different bitwidths for different layers.
The candidates bitwidth for quantization should be defined in the target platform model:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">config</span> <span class="o">=</span> <span class="n">mct</span><span class="o">.</span><span class="n">CoreConfig</span><span class="p">(</span><span class="n">mixed_precision_config</span><span class="o">=</span><span class="n">MixedPrecisionQuantizationConfigV2</span><span class="p">())</span>
</pre></div>
</div>
<p>For mixed-precision set a target KPI object:
Create a KPI object to limit our returned model’s size. Note that this value affects only coefficients
that should be quantized (for example, the kernel of Conv2D in Keras will be affected by this value,
while the bias will not):</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">kpi</span> <span class="o">=</span> <span class="n">mct</span><span class="o">.</span><span class="n">KPI</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">count_params</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.75</span><span class="p">)</span>  <span class="c1"># About 0.75 of the model size when quantized with 8 bits.</span>
</pre></div>
</div>
<p>Pass the model, the representative dataset generator, the configuration and the target KPI to get a
quantized model:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">quantization_info</span><span class="p">,</span> <span class="n">custom_objects</span> <span class="o">=</span> <span class="n">mct</span><span class="o">.</span><span class="n">keras_quantization_aware_training_init</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">repr_datagen</span><span class="p">,</span> <span class="n">kpi</span><span class="p">,</span> <span class="n">core_config</span><span class="o">=</span><span class="n">config</span><span class="p">)</span>
</pre></div>
</div>
<p>Use the quantized model for fine-tuning. For loading the model from file, use the custom_objects dictionary:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_file</span><span class="p">,</span> <span class="n">custom_objects</span><span class="o">=</span><span class="n">custom_objects</span><span class="p">)</span>
</pre></div>
</div>
<p>For more configuration options, please take a look at our <a class="reference external" href="https://sony.github.io/model_optimization/api/api_docs/modules/mixed_precision_quantization_config.html">API documentation</a>.</p>
</dd></dl>

</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="nav-item nav-item-0"><a href="../../../index.html">MCT Documentation: ver 1.6.0</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Keras Quantization Aware Training Model Init</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2022, Sony Semiconductor Israel.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.0.2.
    </div>
  </body>
</html>