{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf96fb4",
   "metadata": {},
   "source": [
    "# Quantization using the Model Compression Toolkit - example in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ed8f02",
   "metadata": {},
   "source": [
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/mnist_notebook/tutorials/example_pytorch_quantization_mnist.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822944a1",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743dbc3d",
   "metadata": {},
   "source": [
    "This quick start guide covers how to use the Model Compression Toolkit (MCT) for quantizing a PyTorch model. We will do so by giving an end-to-end example, training a model from scratch on MNIST data, then quantizing it using the MCT. Let us start with the relevant imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e2eeae",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf577a",
   "metadata": {},
   "source": [
    "In this tutorial we will cover:\n",
    "1. Training a Pytorch model from scratch on MNIST.\n",
    "2. Quantizing the model in a hardware-friendly manner (symmetric quantization, power-of-2 thresholds) using 8-bit activations and weights.\n",
    "3. We will examine the output quantized model, evaluate it and compare its performance to the original model.\n",
    "4. We will approximate the compression gains due to quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3396bf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7690ef",
   "metadata": {},
   "source": [
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q model-compression-toolkit\n",
    "! pip install -q torch \n",
    "! pip install -q torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82928d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import model_compression_toolkit as mct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1653425b",
   "metadata": {},
   "source": [
    "## Train a Pytorch classifier model on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02312089",
   "metadata": {},
   "source": [
    "Let us define the network and some helper functions to train and evaluate the model. These are taken from the official Pytorch examples https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f9bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output\n",
    "\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 1000\n",
    "random_seed = 1\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "dataset_folder = '/Vols/vol_design/tools/swat/users/liord/datasets/mnist'\n",
    "epochs = 2\n",
    "gamma = 0.7\n",
    "lr = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24d3c5a",
   "metadata": {},
   "source": [
    "Let us define the dataset loaders, and optimizer and train the model for 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c615a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "dataset1 = datasets.MNIST(dataset_folder, train=True, download=True,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST(dataset_folder, train=False,\n",
    "                   transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, num_workers=1, pin_memory=True, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, num_workers=1, pin_memory=True,  batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "model = Net().to(device)\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=gamma)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69366614",
   "metadata": {},
   "source": [
    "After training for 2 epochs we get an accuracy of 98.5%. Not bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd25a7",
   "metadata": {},
   "source": [
    "## Hardware-friendly quantization using MCT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0321aad",
   "metadata": {},
   "source": [
    "Now we would like to quantize this model using the Model Compression Toolkit.\n",
    "To do so, we need to define a representative dataset, which is a function that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618975be",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_loader = iter(train_loader)\n",
    "\n",
    "def representative_data_gen() -> list:\n",
    "    return [next(image_data_loader)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a92bee",
   "metadata": {},
   "source": [
    "Now for the fireworks. Lets run hardware-friendly post training quantization on the model. The output of MCT is a simulated quantized model in the input model's framework. That is, the model adds fake-quantization nodes after layers that need to be quantized. The output model's size on the disk does'nt change, but all the quantization parameters are available for deployment on target hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f695dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.pytorch_post_training_quantization(\n",
    "    model, \n",
    "    representative_data_gen, \n",
    "    n_iter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3521637",
   "metadata": {},
   "source": [
    "The MCT prints the approximated model size after real quantization and the compression ratio. In this example, we used the default setting of MCT and compressed the model from 32 bits to 8 bits, hence the compression ratio is x4. Using the simulated quantized model, we can evaluate its performance using the original model's testing environment, and compare its performance to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fa4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(quantization_info)\n",
    "test(quantized_model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09fa27",
   "metadata": {},
   "source": [
    "In this scenario, we see that the compression almost didn't affect the accuracy of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {},
   "source": [
    "In this tutorial, we demonstrated how to quantize a classification model for MNIST in a hardware-friendly manner using MCT. We saw that we can achieve an x4 compression ratio with minimal performance degradation. \n",
    "\n",
    "The advantage of quantizing in a hardware-friendly manner is that this model can run more efficiently in the sense of run time, power consumption, and memory on designated hardware. \n",
    "\n",
    "This is a very simple model and a very simple task. MCT can demonstrate competitive results on a wide variety of tasks and network architectures. Check out the paper for more details: https://arxiv.org/abs/2109.09113"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
