{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# How to Use the Network Editor to Easily Modify Quantization Configurations in the Model Compression Toolkit (MCT)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_network_editor.ipynb)\n",
    "\n",
    "## Overview\n",
    "In this tutorial, we will demonstrate how to utilize the Model Compression Toolkit (MCT) to quantize a simple Keras model and modify the quantization configuration for specific layers using MCT’s network editor. The example model comprises a `Conv2D` layer followed by a `Dense` layer.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Quantizing the model using the default configuration and inspecting bit allocation for each layer.\n",
    "2. Applying a custom edit rule to adjust the bit-width for the `Conv2D` layer.\n",
    "3. Showcasing MCT’s flexibility for layer-specific quantization.\n",
    "\n",
    "## Setup\n",
    "Install and import the relevant packages:"
   ],
   "metadata": {
    "id": "C_BBKEpTRqp_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6LXeaQD1c-w"
   },
   "outputs": [],
   "source": [
    "TF_VER = '2.14'\n",
    "!pip install -q tensorflow[and-cuda]~={TF_VER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import model_compression_toolkit as mct\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense\n",
    "from tensorflow.keras.models import Model"
   ],
   "metadata": {
    "id": "vCsjoKb7168U"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will create a simple Keras model consisting of a `Conv2D` layer followed by a `Dense` layer."
   ],
   "metadata": {
    "id": "bRPoKI-WSQn2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "input_shape = (16, 16, 3)\n",
    "\n",
    "inputs = Input(shape=input_shape)\n",
    "x = Conv2D(filters=1, kernel_size=(3, 3))(inputs)\n",
    "x = Dense(units=10)(x)\n",
    "model = Model(inputs=inputs, outputs=x)"
   ],
   "metadata": {
    "id": "uOu8c7n_6Vd4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Represenatative Dataset\n",
    "In this tutorial, for demonstration purposes and to expedite the process, we create a simple representative dataset generator using random data. This generator produces batches of random input data that match the model’s input shape."
   ],
   "metadata": {
    "id": "rDAMPxKhSYfx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 1\n",
    "def representative_data_gen():\n",
    "    yield [np.random.randn(batch_size, *input_shape)]"
   ],
   "metadata": {
    "id": "LvnQmku02qIM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Quantization with MCT\n",
    "Let’s define a function that takes a Keras model, a representative data generator, and a core configuration for quantization. The function will use the MCT’s post-training quantization (PTQ) API to apply quantization to the model."
   ],
   "metadata": {
    "id": "VecsI-kDe9RM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def quantize_keras_mct(model, representative_data_gen, core_config):\n",
    "  quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "      in_model=model,\n",
    "      representative_data_gen=representative_data_gen,\n",
    "      core_config=core_config\n",
    "  )\n",
    "  return quantized_model"
   ],
   "metadata": {
    "id": "uIyyoMv93Bt7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define a function to inspect the bit-width used for quantizing specific layers. The function retrieves and prints the bit-width for the `kernel` attribute in both the `Conv2D` and `Dense` layers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def print_model_weights_by_layer(model):\n",
    "    conv2d_layer = model.layers[2]\n",
    "    conv2d_nbits = conv2d_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
    "    \n",
    "    dense_layer = model.layers[4]\n",
    "    dense_nbits = dense_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
    "    \n",
    "    print(f\"Conv2D nbits: {conv2d_nbits}, Dense nbits: {dense_nbits}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quantization\n",
    "In this section, we start by setting a default core configuration for quantization using MCT’s `CoreConfig`. With this configuration, the model is quantized using the default 8-bit precision for all layer types. Next, we print the bit-width settings to verify the quantization of both the Conv2D and Dense layers."
   ],
   "metadata": {
    "id": "Xqmg7vWNgsqc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Use default core config for observing baseline quantized model\n",
    "core_config = mct.core.CoreConfig()\n",
    "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config)\n",
    "print_model_weights_by_layer(quantized_model)"
   ],
   "metadata": {
    "id": "Z5VDv6Bz4cqN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Edit Configration Using Edit Rules List\n",
    "\n",
    " Now, let's customize the quantization process for specific layers using MCT’s network editor. We create an `EditRule` with a `NodeTypeFilter` targeting the `Conv2D` layer type.\n",
    "\n",
    "The associated action changes the kernel attribute’s bit-width to 4 bits instead of the default 8 bits. This rule is then added to an `edit_rules_list`, which is passed to `DebugConfig`.\n",
    "\n",
    "The custom `DebugConfig` is used to create a `CoreConfig`, enabling `Conv2D` layers to be quantized at 4 bits while other layers retain the default configuration."
   ],
   "metadata": {
    "id": "FyBwtQuMhQMt"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "edit_rules_list = [\n",
    "    mct.core.network_editor.EditRule(\n",
    "        filter=mct.core.network_editor.NodeTypeFilter(Conv2D),\n",
    "        action=mct.core.network_editor.ChangeCandidatesWeightsQuantConfigAttr(attr_name='kernel', weights_n_bits=4)\n",
    "    )\n",
    "]\n",
    "\n",
    "debug_config = mct.core.DebugConfig(network_editor=edit_rules_list)\n",
    "core_config_edit_weight_bits = mct.core.CoreConfig(debug_config=debug_config)"
   ],
   "metadata": {
    "id": "7YynVSSh3Mk-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will apply this customized quantization configuration to the Keras model.\n",
    "\n",
    "By calling `quantize_keras_mct` with the `core_config` containing our edit rule, we quantize the `Conv2D` layer using 4 bits as specified. The resulting `quantized_model` reflects these changes, which we verify by inspecting the bit-width used in both the `Conv2D` and `Dense` layers.\n",
    "\n",
    "The output confirms the effect of the edit rule: the `Conv2D` layer is quantized with 4 bits, while the `Dense` layer retains the default 8-bit setting."
   ],
   "metadata": {
    "id": "ftkeDjZPiahd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config_edit_weight_bits)\n",
    "print_model_weights_by_layer(quantized_model)"
   ],
   "metadata": {
    "id": "7p6qFWoEQBS5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Edit Z-Threshold for Activation Quantization\n",
    "In model quantization, the Z-Threshold helps manage outliers in activation data, which can negatively impact the efficiency and accuracy of the quantization process. It sets a boundary to exclude extreme values when determining quantization parameters, improving robustness and model performance.\n",
    "\n",
    "Adjusting the Z-Threshold is useful for fine-tuning model accuracy and handling outliers. A higher Z-Threshold includes more data, potentially accounting for outliers, while a lower value effectively filters them out.\n",
    "\n",
    "The following code demonstrates how to customize the Z-Threshold for specific layer types, such as `Conv2D`, using MCT’s network editor. By default, all layers have an infinite threshold, meaning no outlier removal occurs."
   ],
   "metadata": {
    "id": "2TqXTB48jKHx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "z_threshold_target = 5\n",
    "edit_rules_list = [\n",
    "    mct.core.network_editor.EditRule(\n",
    "        filter=mct.core.network_editor.NodeTypeFilter(Conv2D),\n",
    "        action=mct.core.network_editor.ChangeCandidatesActivationQuantConfigAttr(z_threshold=z_threshold_target)\n",
    "    )\n",
    "]\n",
    "\n",
    "debug_config = mct.core.DebugConfig(network_editor=edit_rules_list)\n",
    "core_config_edit_z_threshold = mct.core.CoreConfig(debug_config=debug_config)\n",
    "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config_edit_z_threshold)"
   ],
   "metadata": {
    "id": "VBRfQqZVjN3J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "In this tutorial, we explored how to leverage the Model Compression Toolkit (MCT) for quantizing Keras models and customizing the quantization configuration for specific layers using the network editor. We started by applying the default 8-bit quantization and inspecting the results. Then, we demonstrated how to use the network editor to modify the bit-width for individual layers and fine-tune activation quantization using Z-Threshold adjustments.\n",
    "\n",
    "\n",
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ],
   "metadata": {
    "id": "A1rhMoGUji1e"
   }
  }
 ]
}
