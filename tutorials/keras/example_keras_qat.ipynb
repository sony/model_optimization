{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09f31596-c293-4faa-8253-336769f8faa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Quantization Aware Training using the Model Compression Toolkit - example in Keras\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_qat.ipynb)\n",
    "## Overview\n",
    "This tutorial will demonstrate how to use the Quantization Aware Training (QAT) API of the Model Compression Toolkit (MCT). We will train a neural network on the MNIST dataset and apply quantization using the MCT QAT API to optimize the model for efficient hardware deployment without sacrificing accuracy.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. **Training a Keras model on MNIST:** We'll begin by constructing a simple neural network and training it on the MNIST dataset. \n",
    "2. **Configuring Target Platform Capabilities (TPC):** Define the quantization settings for weights and activations.\n",
    "3. **Preparing the Model for QAT:** Convert the floating-point model into a QAT-ready model using MCT. \n",
    "4. **Training the Model with QAT:**  Perform quantization-aware training to preserve model accuracy.\n",
    "5. **Evaluating and Exporting the Quantized Model:** Finalize and export the optimized quantized model for deployment.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b380c492-3c53-4ec1-987e-de693a1ec1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_VER = '2.14.0'\n",
    "!pip install -q tensorflow[and-cuda]~={TF_VER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee8ecbde24b55fbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c27b1-65f9-4fd3-be3e-733f4c60124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import Model, layers, datasets\n",
    "import model_compression_toolkit as mct\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Preprocessing MNIST\n",
    "Let's define the dataset loaders to retrieve the train and test parts of the MNIST dataset, including preprocessing:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2c524e5d4985e47"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n",
    "\n",
    "# Add Channels axis to data\n",
    "train_images = np.expand_dims(train_images, -1)\n",
    "test_images = np.expand_dims(test_images, -1)\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "train_labels = tf.keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "729d91394f1ded4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Keras Model\n",
    "In this section, we create a simple Keras model to demonstrate the QAT process. The model consists of two convolutional layers, two dense layers, and dropout layers for regularization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb31c629993369f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    _input = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(16, 3, strides=2, padding='same', activation='relu')(_input)\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=_input, outputs=x)\n",
    "    model.summary()\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c6f97097cbf81f9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training the Model on MNIST\n",
    "Next, we will train the dense model using the preprocessed MNIST dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a31cff183889d31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "batch_size = 128\n",
    "\n",
    "# Train and evaluate the model\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(test_images, test_labels))\n",
    "model.evaluate(test_images, test_labels)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e6f320b23217c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing the Model for Hardware-Friendly Quantization Aware Training with MCT\n",
    "## Target Platform Capabilities\n",
    "MCT optimizes the model for dedicated hardware. This is done using TPC (for more details, please visit our [documentation](https://sony.github.io/model_optimization/docs/api/api_docs/modules/target_platform.html)). In this tutorial, we use a TPC configuration that applies 2-bit quantization for weights and 3-bit quantization for activations.\n",
    "\n",
    "If desired, you can skip this step and directly use the pre-configured [`get_target_platform_capabilities`](https://sony.github.io/model_optimization/docs/api/api_docs/methods/get_target_platform_capabilities.html) function to obtain an initialized TPC."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f7e6ec541426aa5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6f84b-9775-4989-9f74-688958f3a1d3",
   "metadata": {
    "id": "8bb6f84b-9775-4989-9f74-688958f3a1d3"
   },
   "outputs": [],
   "source": [
    "from model_compression_toolkit import DefaultDict\n",
    "from model_compression_toolkit.target_platform_capabilities.target_platform.op_quantization_config import AttributeQuantizationConfig, Signedness\n",
    "from model_compression_toolkit.constants import FLOAT_BITWIDTH\n",
    "from model_compression_toolkit.target_platform_capabilities.constants import KERNEL_ATTR, KERAS_KERNEL, BIAS_ATTR, BIAS\n",
    "\n",
    "tp = mct.target_platform\n",
    "\n",
    "\n",
    "def get_tpc():\n",
    "    \"\"\"\n",
    "    Assuming a target hardware that uses a power-of-2 threshold for activations and\n",
    "    a symmetric threshold for the weights. The activations are quantized to 3 bits, and the kernel weights\n",
    "    are quantized to 2 bits. Our assumed hardware does not require quantization of some layers\n",
    "    (e.g. Flatten & Droupout).\n",
    "    This function generates a TargetPlatformCapabilities with the above specification.\n",
    "\n",
    "    Returns:\n",
    "         TargetPlatformCapabilities object\n",
    "    \"\"\"\n",
    "\n",
    "    # define a default quantization config for all non-specified weights attributes.\n",
    "    default_weight_attr_config = AttributeQuantizationConfig(\n",
    "        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,\n",
    "        weights_n_bits=8,\n",
    "        weights_per_channel_threshold=False,\n",
    "        enable_weights_quantization=False,\n",
    "        lut_values_bitwidth=None)\n",
    "\n",
    "    # define a quantization config to quantize the kernel (for layers where there is a kernel attribute).\n",
    "    kernel_base_config = AttributeQuantizationConfig(\n",
    "        weights_quantization_method=tp.QuantizationMethod.SYMMETRIC,\n",
    "        weights_n_bits=2,\n",
    "        weights_per_channel_threshold=True,\n",
    "        enable_weights_quantization=True,\n",
    "        lut_values_bitwidth=None)\n",
    "\n",
    "    # define a quantization config to quantize the bias (for layers where there is a bias attribute).\n",
    "    bias_config = AttributeQuantizationConfig(\n",
    "        weights_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,\n",
    "        weights_n_bits=FLOAT_BITWIDTH,\n",
    "        weights_per_channel_threshold=False,\n",
    "        enable_weights_quantization=False,\n",
    "        lut_values_bitwidth=None)\n",
    "\n",
    "    # Create a default OpQuantizationConfig where we use default_weight_attr_config as the default\n",
    "    # AttributeQuantizationConfig for weights with no specific AttributeQuantizationConfig.\n",
    "    # MCT will compress a layer's kernel and bias according to the configurations that are\n",
    "    # set in KERNEL_ATTR and BIAS_ATTR that are passed in attr_weights_configs_mapping.\n",
    "    default_config = tp.OpQuantizationConfig(\n",
    "        default_weight_attr_config=default_weight_attr_config,\n",
    "        attr_weights_configs_mapping={KERNEL_ATTR: kernel_base_config,\n",
    "                                      BIAS_ATTR: bias_config},\n",
    "        activation_quantization_method=tp.QuantizationMethod.POWER_OF_TWO,\n",
    "        activation_n_bits=3,\n",
    "        supported_input_activation_n_bits=8,\n",
    "        enable_activation_quantization=True,\n",
    "        quantization_preserving=False,\n",
    "        fixed_scale=None,\n",
    "        fixed_zero_point=None,\n",
    "        simd_size=None,\n",
    "        signedness=Signedness.AUTO)\n",
    "\n",
    "    # Set default QuantizationConfigOptions in new TargetPlatformModel to be used when no other\n",
    "    # QuantizationConfigOptions is set for an OperatorsSet.\n",
    "    default_configuration_options = tp.QuantizationConfigOptions([default_config])\n",
    "    tp_model = tp.TargetPlatformModel(default_configuration_options)\n",
    "    with tp_model:\n",
    "        default_qco = tp.get_default_quantization_config_options()\n",
    "        # Group of OperatorsSets that should not be quantized.\n",
    "        tp.OperatorsSet(\"NoQuantization\",\n",
    "                        default_qco.clone_and_edit(enable_activation_quantization=False)\n",
    "                        .clone_and_edit_weight_attribute(enable_weights_quantization=False))\n",
    "        # Group of linear OperatorsSets such as convolution and matmul.\n",
    "        tp.OperatorsSet(\"LinearOp\")\n",
    "\n",
    "    tpc = tp.TargetPlatformCapabilities(tp_model)\n",
    "    with tpc:\n",
    "        # No need to quantize Flatten and Dropout layers\n",
    "        tp.OperationsSetToLayers(\"NoQuantization\", [layers.Flatten, layers.Dropout])\n",
    "        # Assign the framework layers' attributes to KERNEL_ATTR and BIAS_ATTR that were used during creation\n",
    "        # of the default OpQuantizationConfig.\n",
    "        tp.OperationsSetToLayers(\"LinearOp\", [layers.Dense, layers.Conv2D],\n",
    "                                 attr_mapping={KERNEL_ATTR: DefaultDict(default_value=KERAS_KERNEL),\n",
    "                                               BIAS_ATTR: DefaultDict(default_value=BIAS)})\n",
    "    return tpc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a73bebf51aaf672c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "\n",
    "def representative_data_gen():\n",
    "    def _generator():\n",
    "        for _ind in range(n_iter):\n",
    "            yield [train_images[_ind][np.newaxis, ...]]\n",
    "    return _generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a82f6b9bae8269a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating a QAT-Ready Model with MCT\n",
    "The MCT converts a floating-point model into a quantized model using post-training quantization. The returned model includes trainable quantizers and is ready for fine-tuning, making it a \"QAT-ready\" model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4a247d3bde88990"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c171d2d-6f0d-474d-aab6-22b0b0c9e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model, _, custom_objects = mct.qat.keras_quantization_aware_training_init_experimental(\n",
    "    model,\n",
    "    representative_data_gen(),\n",
    "    target_platform_capabilities=get_tpc())\n",
    "qat_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"], run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets evaluate the performance after the basic post-trainig quantization."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38ba814d794cd57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score = qat_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"PTQ model test accuracy: {score[1]:02.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "976a5942ac5fa9c6"
  },
  {
   "cell_type": "markdown",
   "id": "78c74675-b4b5-42bd-a0b7-75da240cbf66",
   "metadata": {},
   "source": [
    "## User Quantization Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f1d23-9610-415a-84c2-8ef953370574",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model.fit(train_images, train_labels, epochs=epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "score = qat_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"QAT model test accuracy: {score[1]:02.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finalizing the QAT model: \n",
    "Remove the 'QuantizeWrapper' layers to retain only the layers with quantized weights (FakeQuant values)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d34761c03296a31a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856de5a6-29d6-4e65-80b1-9f11ed63ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = mct.qat.keras_quantization_aware_training_finalize_experimental(qat_model)\n",
    "\n",
    "quantized_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "score = quantized_model.evaluate(test_images, test_labels, verbose=0)\n",
    "print(f\"Quantized model test accuracy: {score[1]:02.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can export the quantized model to Keras:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52158ce9c8ded4bb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856de5a6-29d6-4e65-80b1-9f11ed63ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path='qmodel.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "In this tutorial, we explored how to perform Quantization Aware Training (QAT) using the Model Compression Toolkit (MCT) with a Keras model. We began by constructing a simple neural network and preparing it for quantization by configuring the Target Platform Capabilities (TPC). Then, we converted the model into a QAT-ready format and demonstrated how to train and fine-tune it using hardware-friendly quantization settings. This approach can significantly reduce the model size and improve inference speed while maintaining high accuracy, making it ideal for edge AI applications.\n",
    "\n",
    "Feel free to experiment with different configurations to see how they impact your models."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c4089ae72fb2c6d"
  },
  {
   "cell_type": "markdown",
   "id": "db77d678-1fa7-4dc0-a6f3-bac10ba2d8ed",
   "metadata": {},
   "source": [
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
