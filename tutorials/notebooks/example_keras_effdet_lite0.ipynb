{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Post Training Quantization an EfficientDet Object Detection Model\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/example_keras_effdet_lite0.ipynb)\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we'll demonstrate the post-training quantization using MCT for a pre-trained object detection model in Keras. In addition, we'll integrate a post-processing custom layer from [sony-custom-layers](https://github.com/sony/custom_layers) into the model. This integration aligns with the imx500 target platform capabilities.\n",
    "\n",
    "In this example we will use an existing pre-trained EfficientDet model taken from [efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch). We will convert the model to a Keras functional model that includes the custom [PostProcess Layer](https://github.com/sony/custom_layers/blob/main/sony_custom_layers/keras/object_detection/ssd_post_process.py). Further, we will quantize the model using MCT post training quantization and evaluate the performance of the floating point model and the quantized model on the COCO dataset.\n",
    "\n",
    "We'll use the [timm](https://github.com/huggingface/pytorch-image-models)'s data loader and evaluation capabilities used for the original pytorch pretrained model. The conversion to the Keras model will not be covered. You can go over the conversion [here](https://github.com/sony/model_optimization/tree/main/tutorials/resources/efficientdet)\n",
    "\n",
    "Steps:\n",
    "* **Setup environment**: install relevant packages, import them\n",
    "* **Init dataset**: Download the COCO evaluation and prepare the evaluation code\n",
    "* **Keras float model**: Create the Keras model, assign the pretrained weights and evaluate it\n",
    "* **Quantize Keras mode**: Quantize the model and evaluate it\n",
    "\n",
    "**Note**: The following code should be run on a GPU."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9e7b10d2bfe67d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "install and import relevant packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0e81b09e6d30873"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q tensorflow\n",
    "!pip install -q model-compression-toolkit\n",
    "!pip install -q torch\n",
    "!pip install -q torchvision\n",
    "!pip install -q timm\n",
    "!pip install -q effdet\n",
    "!pip install -q sony-custom-layers"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6695a3ec84402e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from time import time\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from timm.utils import AverageMeter\n",
    "from effdet.anchors import Anchors\n",
    "from effdet.config import get_efficientdet_config\n",
    "from effdet import create_dataset, create_loader, create_evaluator\n",
    "from effdet.data import resolve_input_config\n",
    "import model_compression_toolkit as mct\n",
    "from sony_custom_layers.keras.object_detection.ssd_post_process import SSDPostProcess\n",
    "from sony_custom_layers.keras.object_detection import ScoreConverter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "735aee910cf92d42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to convert the PyTorch model, you'll need to use the conversion code in the [MCT tutorials folder](https://github.com/sony/model_optimization/tree/main/tutorials), so we'll clone the MCT repository to a local folder and only use that code. The installed MCT package will be used for quantization. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda6ab0d8f0b6b56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/sony/model_optimization.git local_mct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca9a743c0e7ba067"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,\"/content/local_mct\")\n",
    "from tutorials.resources.efficientdet import EfficientDetKeras\n",
    "from tutorials.resources.utils import load_state_dict"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e460c939d89482"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Init dataset\n",
    "\n",
    "### Load COCO evaluation set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f75abdac7950c038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!wget -nc http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "!unzip -q -o annotations_trainval2017.zip -d /content/coco\n",
    "!echo Done loading annotations\n",
    "!wget -nc http://images.cocodataset.org/zips/val2017.zip\n",
    "!unzip -q -o val2017.zip -d /content/coco\n",
    "!echo Done loading val2017 images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf50c7706331ba8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init data loader and evaluation functions\n",
    "\n",
    "These functions were adapted from the [efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch) repository."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6010ecf194d4a6a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TorchWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A class to wrap the EfficientDet Keras model in a torch.nn.Module so it can be evaluated with timm's evaluation code\n",
    "    \"\"\"\n",
    "    def __init__(self, keras_model: tf.keras.Model):\n",
    "        super(TorchWrapper, self).__init__()\n",
    "        self.model = keras_model\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        # a property used by the evaluation code\n",
    "        return self.model.config\n",
    "\n",
    "    def forward(self, x: torch.Tensor, img_info: Optional[Dict[str, torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        mimics the forward inputs of the EfficientDet PyTorch model.\n",
    "        Args:\n",
    "            x: inputs images\n",
    "            img_info: input image info for scaling the outputs\n",
    "\n",
    "        Returns:\n",
    "            A torch.Tensor of shape [Batch, Boxes, 6], the same as the PyTorch model\n",
    "\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        keras_input = x.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
    "        outputs = self.model(keras_input)\n",
    "\n",
    "        outs = [torch.Tensor(o.numpy()).to(device) for o in outputs]\n",
    "        outs[0] = outs[0][:, :, [1, 0, 3, 2]]  # reorder (y, x, y2, x2) to (x, y, x2, y2)\n",
    "        outs[0] = outs[0] * img_info['img_scale'].view((-1, 1, 1))  # scale to original image size\n",
    "        return torch.cat([outs[0], outs[1].unsqueeze(2), outs[2].unsqueeze(2) + 1], 2)\n",
    "\n",
    "\n",
    "def get_coco_dataloader(batch_size=16, split='val', config=None):\n",
    "    \"\"\"\n",
    "        Get the torch data-loader and evaluation object\n",
    "    Args:\n",
    "        batch_size: batch size for data loader\n",
    "        split: dataset split\n",
    "        config: model config\n",
    "\n",
    "    Returns:\n",
    "        The DataLoader and evaluation object for calculating accuracy\n",
    "\n",
    "    \"\"\"\n",
    "    root = '/content/coco'\n",
    "\n",
    "    args = dict(interpolation='bilinear', mean=None, std=None, fill_color=None)\n",
    "    dataset = create_dataset('coco', root, split)\n",
    "    input_config = resolve_input_config(args, config)\n",
    "    loader = create_loader(\n",
    "        dataset,\n",
    "        input_size=input_config['input_size'],\n",
    "        batch_size=batch_size,\n",
    "        use_prefetcher=True,\n",
    "        interpolation=input_config['interpolation'],\n",
    "        fill_color=input_config['fill_color'],\n",
    "        mean=input_config['mean'],\n",
    "        std=input_config['std'],\n",
    "        num_workers=0,\n",
    "        pin_mem=False,\n",
    "    )\n",
    "    evaluator = create_evaluator('coco', dataset, pred_yxyx=False)\n",
    "\n",
    "    return loader, evaluator\n",
    "\n",
    "\n",
    "def acc_eval(_model: tf.keras.Model, batch_size=16, config=None):\n",
    "    \"\"\"\n",
    "    This function takes a Keras model, wraps it in a Torch model and runs evaluation\n",
    "    Args:\n",
    "        _model: Keras model\n",
    "        batch_size: batch size of the data loader\n",
    "        config: model config\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # wrap Keras model in a Torch model so it can run in timm's evaluation code\n",
    "    _model = TorchWrapper(_model)\n",
    "    # EValuate input model\n",
    "    val_loader, evaluator = get_coco_dataloader(batch_size=batch_size, config=config)\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    end = time()\n",
    "    last_idx = len(val_loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            output = _model(input, img_info=target)\n",
    "\n",
    "            evaluator.add_predictions(output, target)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time() - end)\n",
    "            end = time()\n",
    "            if i % 10 == 0 or i == last_idx:\n",
    "                print(\n",
    "                    f'Test: [{i:>4d}/{len(val_loader)}]  '\n",
    "                    f'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {input.size(0) / batch_time.avg:>7.2f}/s)  '\n",
    "                )\n",
    "\n",
    "    return evaluator.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5833c805a1ca77aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras model\n",
    "\n",
    "Create the Keras model and copy weights from pretrained PyTorch weights file. Saved as \"model.keras\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e589b01c6a45a9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def add_postprocess(model: tf.keras.Model, config) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Build a Keras model with the sony custom PostProcess layer\n",
    "    Args:\n",
    "        model: A keras backbone+head model\n",
    "        config: model config\n",
    "\n",
    "    Returns:\n",
    "        A keras model with PostProcess layer\n",
    "\n",
    "    \"\"\"\n",
    "    input_shape = [*config.image_size] + [3]\n",
    "    _input = tf.keras.layers.Input(shape=input_shape)\n",
    "    x_box, x_class = model(_input)\n",
    "    anchors = tf.constant(Anchors.from_config(config).boxes.detach().cpu().numpy())\n",
    "    ssd_pp = SSDPostProcess(anchors, [1, 1, 1, 1], [*config.image_size],\n",
    "                            ScoreConverter.SIGMOID, score_threshold=0.001, iou_threshold=0.5,\n",
    "                            max_detections=config.max_det_per_image)\n",
    "    outputs = ssd_pp((x_box, x_class))\n",
    "\n",
    "    return tf.keras.Model(inputs=_input, outputs=outputs)\n",
    "\n",
    "\n",
    "model_name = 'tf_efficientdet_lite0'\n",
    "config = get_efficientdet_config(model_name)\n",
    "\n",
    "model = EfficientDetKeras(config, pretrained_backbone=False).get_model([*config.image_size] + [3])\n",
    "\n",
    "model_with_postprocess = add_postprocess(model, config)\n",
    "model_with_postprocess.save('/content/model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f1dacee7a949928"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate Keras model\n",
    "\n",
    "Wrap model in a Torch Module, so it can be evaluated with timm's evaluation code. We evaluate the model to verify the conversion succeeded and to compare it to the quantized model evaluation. The \"TorchWrapper\" object is a PyTorch module that serves as an API between the timm's Torch evaluation framework and the Keras model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef6b474a69358e03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "float_map = acc_eval(model_with_postprocess, batch_size=64, config=config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2c87ab3460f395"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantized Keras model\n",
    "\n",
    "The quantized model is saved as \"quant_model.keras\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aca80a0fc370eef3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loader, _ = get_coco_dataloader(split='val', config=config)\n",
    "\n",
    "\n",
    "def get_representative_dataset(n_iter):\n",
    "    \"\"\"\n",
    "    This function creates a representative dataset generator\n",
    "    Args:\n",
    "        n_iter: number of iterations for MCT to calibrate on\n",
    "\n",
    "    Returns:\n",
    "        A representative dataset generator\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def representative_dataset():\n",
    "        \"\"\"\n",
    "        Creates a representative dataset generator from a PyTorch data loader, The generator yields numpy\n",
    "        arrays of batches of shape: [Batch, H, W ,C]\n",
    "        Returns:\n",
    "            A representative dataset generator\n",
    "\n",
    "        \"\"\"\n",
    "        ds_iter = iter(loader)\n",
    "        for _ in range(n_iter):\n",
    "            t = next(ds_iter)[0]\n",
    "            # Convert the Torch tensor from the data loader to a numpy array and transpose to the\n",
    "            # right shape: [B, C, H, W] -> [B, H, W, C]\n",
    "            tf_shaped_tensor = t.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
    "            yield [tf_shaped_tensor]\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "\n",
    "# Set IMX500-v1 TPC\n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
    "# set weights compression target, so the quantized model will fit the IMX500 memory\n",
    "kpi = mct.KPI(weights_memory=2674291)\n",
    "# set MixedPrecision configuration for compressing the weights\n",
    "mp_config = mct.core.MixedPrecisionQuantizationConfigV2(use_grad_based_weights=False)\n",
    "core_config = mct.core.CoreConfig(mixed_precision_config=mp_config)\n",
    "quant_model, _ = mct.ptq.keras_post_training_quantization_experimental(model,\n",
    "                                                                       get_representative_dataset(20),\n",
    "                                                                       target_kpi=kpi,\n",
    "                                                                       core_config=core_config,\n",
    "                                                                       target_platform_capabilities=tpc)\n",
    "quant_model.save('/content/quant_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f1fa147c5a16df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate quantized Keras model\n",
    "\n",
    "Quantized Keras model evaluation applied the same as the original model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ae299b0b019953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quant_map = acc_eval(quant_model_with_postprocess, batch_size=64, config=config)\n",
    "\n",
    "print(f' ===>> Float model mAP = {100*float_map:2.3f}, Quantized model mAP = {100*quant_map:2.3f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f93b9b932fb39cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d36d177779d29347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
