{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Network Editor Usage\n",
        "\n",
        "[Run this tutorial in Google Colab](https://colab.research.google.com/github/reuvenperetz/model_optimization/blob/add-network-editor-notebook/tutorials/notebooks/example_keras_network_editor.ipynb)\n",
        "\n",        
        "## Introduction\n",
        "\n",
        "In this tutorial, we will demonstrate how to leverage the Model Compression Toolkit (MCT) to quantize a simple Keras model and modify the quantization configuration for specific layers using the MCT's network editor. Our example model consists of a Conv2D layer followed by a Dense layer. Initially, we will quantize this model with a default configuration and inspect the bit allocation for each layer's weights. Then, we will introduce an edit rule to specifically quantize the Conv2D layer with a different bit width, showcasing the flexibility of MCT in customizing quantization schemes per layer.\n",
        "\n",
        "First, we install MCT and import requiered modules:"
      ],
      "metadata": {
        "id": "C_BBKEpTRqp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6LXeaQD1c-w"
      },
      "outputs": [],
      "source": [
        "! pip install -q model-compression-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import model_compression_toolkit as mct\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "vCsjoKb7168U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we create a simple Keras model with a Conv2D layer and a Dense layer:"
      ],
      "metadata": {
        "id": "bRPoKI-WSQn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (16, 16, 3)\n",
        "\n",
        "inputs = Input(shape=input_shape)\n",
        "x = Conv2D(filters=1, kernel_size=(3, 3))(inputs)\n",
        "x = Dense(units=10)(x)\n",
        "model = Model(inputs=inputs, outputs=x)"
      ],
      "metadata": {
        "id": "uOu8c7n_6Vd4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, for demonstration purposes and to expedite the process, we create a simple representative dataset generator using random data. This generator produces a batch of random input data matching the model's input shape."
      ],
      "metadata": {
        "id": "rDAMPxKhSYfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1\n",
        "def representative_data_gen():\n",
        "    yield [np.random.randn(batch_size, *input_shape)]\n"
      ],
      "metadata": {
        "id": "LvnQmku02qIM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function that takes a Keras model, a representative data generator, and a core configuration for quantization. The function utilizes Model Compression Toolkit's post-training quantization API:"
      ],
      "metadata": {
        "id": "VecsI-kDe9RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def quantize_keras_mct(model, representative_data_gen, core_config):\n",
        "  quantized_model, quantization_info = mct.ptq.keras_post_training_quantization_experimental(\n",
        "      in_model=model,\n",
        "      representative_data_gen=representative_data_gen,\n",
        "      core_config=core_config\n",
        "  )\n",
        "  return quantized_model\n"
      ],
      "metadata": {
        "id": "uIyyoMv93Bt7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we start by setting a default core configuration for quantization using Model Compression Toolkit's CoreConfig. After quantizing the model with this configuration, we examine the number of bits used in the quantization of specific layers. We retrieve and print the number of bits used for the the layers' attribute called 'kernel' in both the Conv2D layer and the Dense layer. By default 8-bit are used for quantization across different types of layers in a model."
      ],
      "metadata": {
        "id": "Xqmg7vWNgsqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use default core config for observing baseline quantized model\n",
        "core_config = mct.core.CoreConfig()\n",
        "\n",
        "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config)\n",
        "conv2d_layer = quantized_model.layers[2]\n",
        "conv2d_nbits = conv2d_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
        "\n",
        "dense_layer = quantized_model.layers[4]\n",
        "dense_nbits = dense_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
        "\n",
        "print(f\"Conv2D nbits: {conv2d_nbits}, Dense nbits: {dense_nbits}\")"
      ],
      "metadata": {
        "id": "Z5VDv6Bz4cqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edit Configration Using Edit Rules List\n",
        "\n",
        " Now let's see how to customize the quantization process for specific layers using MCT's network editor. An `EditRule` is created with a `NodeTypeFilter` targeting the Conv2D layer type.\n",
        "\n",
        "  The action associated with this rule changes the quantization configuration of the weights to 4 bits instead of the default 8 bits. This rule is then included in a list (`edit_rules_list`) which is passed to the `DebugConfig`.\n",
        "   \n",
        " The `DebugConfig`, with our custom rule, is then used to create a `CoreConfig`. This configuration will be applied when quantizing the model, resulting in the Conv2D layers being quantized using 4 bits while other layers follow the default setting."
      ],
      "metadata": {
        "id": "FyBwtQuMhQMt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "edit_rules_list = [\n",
        "    mct.network_editor.EditRule(\n",
        "        filter=mct.network_editor.NodeTypeFilter(Conv2D),\n",
        "        action=mct.network_editor.ChangeCandidatesWeightsQuantConfigAttr(weights_n_bits=4)\n",
        "    )\n",
        "]\n",
        "\n",
        "debug_config = mct.DebugConfig(network_editor=edit_rules_list)\n",
        "core_config = mct.core.CoreConfig(debug_config=debug_config)"
      ],
      "metadata": {
        "id": "7YynVSSh3Mk-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this final part of the tutorial, we apply the customized quantization process to our Keras model.\n",
        "\n",
        "By calling `quantize_keras_mct` with the `core_config` containing our edit rule, we specifically quantize the Conv2D layer using 4 bits, as per our custom configuration.\n",
        "\n",
        "The `quantized_model` now reflects these changes. We then extract and display the number of bits used for quantization in both the Conv2D and Dense layers.\n",
        "\n",
        "The output demonstrates the effect of our edit rule: the Conv2D layer is quantized with 4 bits while the Dense layer retains the default 8-bit quantization."
      ],
      "metadata": {
        "id": "ftkeDjZPiahd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model = quantize_keras_mct(model, representative_data_gen, core_config)\n",
        "conv2d_layer = quantized_model.layers[2]\n",
        "conv2d_nbits = conv2d_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
        "\n",
        "dense_layer = quantized_model.layers[4]\n",
        "dense_nbits = dense_layer.weights_quantizers['kernel'].get_config()['num_bits']\n",
        "\n",
        "print(f\"Conv2D nbits: {conv2d_nbits}, Dense nbits: {dense_nbits}\")"
      ],
      "metadata": {
        "id": "7p6qFWoEQBS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n"
      ],
      "metadata": {
        "id": "A1rhMoGUji1e"
      }
    }
  ]
}
