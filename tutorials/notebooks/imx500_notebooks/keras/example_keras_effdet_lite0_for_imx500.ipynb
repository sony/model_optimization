{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Post Training Quantization an EfficientDet Object Detection Model\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/imx500_notebooks/keras/example_keras_effdet_lite0_for_imx500.ipynb)\n",
    "\n",
    "## Overview \n",
    "\n",
    "In this notebook, we'll demonstrate the post-training quantization using MCT for a pre-trained object detection model in Keras. In addition, we'll integrate a post-processing custom layer from [sony-custom-layers](https://github.com/sony/custom_layers) into the model. This custom layer is supported by the imx500 target platform capabilities.\n",
    "\n",
    "In this example we will use an existing pre-trained EfficientDet model taken from [efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch). We will convert the model to a Keras functional model that includes the custom [PostProcess Layer](https://github.com/sony/custom_layers/blob/main/sony_custom_layers/keras/object_detection/ssd_post_process.py). Further, we will quantize the model using MCT post training quantization and evaluate the performance of the floating point model and the quantized model on the COCO dataset.\n",
    "\n",
    "We'll use the [timm](https://github.com/huggingface/pytorch-image-models)'s data loader and evaluation capabilities used for the original PyTorch pretrained model. The conversion to the Keras model will not be covered. You can go over the conversion [here](https://github.com/sony/model_optimization/tree/main/tutorials/mct_model_garden/models_keras/efficientdet).\n",
    "\n",
    "Steps:\n",
    "* **Setup the environment**: install relevant packages, import them\n",
    "* **Initialize the dataset**: Download the COCO evaluation set and prepare the evaluation code\n",
    "* **Keras float model**: Create the Keras model, assign the pretrained weights and evaluate it\n",
    "* **Quantize Keras mode**: Quantize the model and evaluate it\n",
    "\n",
    "**Note**: The following code should be run on a GPU."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c9e7b10d2bfe67d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup\n",
    "\n",
    "install and import relevant packages"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d0e81b09e6d30873"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q tensorflow==2.15.*\n",
    "!pip install -q torchvision\n",
    "!pip install -q timm==0.9.16\n",
    "!pip install -q effdet\n",
    "!pip install -q sony-custom-layers\n",
    "!pip install -q torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6695a3ec84402e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import Dict, Optional\n",
    "from time import time\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from timm.utils import AverageMeter\n",
    "from effdet.config import get_efficientdet_config\n",
    "from effdet import create_dataset, create_loader, create_evaluator\n",
    "from effdet.data import resolve_input_config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "735aee910cf92d42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Install model_compression_toolkit (MCT)**\n",
    "Here we install the model compression toolkit (if it's not already installed). Additionally, in order to convert the PyTorch model, we'll need to use the conversion code in the [MCT tutorials folder](https://github.com/sony/model_optimization/tree/main/tutorials). We copy this folder and add it to the python path. \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eda6ab0d8f0b6b56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "import os\n",
    "\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit\n",
    "!git clone https://github.com/sony/model_optimization.git temp_mct && mv temp_mct/tutorials . && \\rm -rf temp_mct\n",
    "sys.path.insert(0,\"tutorials\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38e460c939d89482"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tutorials.mct_model_garden.models_keras.efficientdet import EfficientDetKeras"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7461504d6590519a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initialize dataset\n",
    "\n",
    "### Load the COCO evaluation set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f75abdac7950c038"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.isdir('coco'):\n",
    "    !wget -nc http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
    "    !unzip -q -o annotations_trainval2017.zip -d ./coco\n",
    "    !echo Done loading annotations\n",
    "    !wget -nc http://images.cocodataset.org/zips/val2017.zip\n",
    "    !unzip -q -o val2017.zip -d ./coco\n",
    "    !echo Done loading val2017 images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1bf50c7706331ba8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the data loader and evaluation functions\n",
    "\n",
    "These functions were adapted from the [efficientdet-pytorch](https://github.com/rwightman/efficientdet-pytorch) repository."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6010ecf194d4a6a1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class TorchWrapper(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A class to wrap the EfficientDet Keras model in a torch.nn.Module\n",
    "    so it can be evaluated with timm's evaluation code\n",
    "    \"\"\"\n",
    "    def __init__(self, keras_model: tf.keras.Model):\n",
    "        super(TorchWrapper, self).__init__()\n",
    "        self.model = keras_model\n",
    "\n",
    "    @property\n",
    "    def config(self):\n",
    "        # a property used by the evaluation code\n",
    "        return self.model.config\n",
    "\n",
    "    def forward(self, x: torch.Tensor,\n",
    "                img_info: Optional[Dict[str, torch.Tensor]] = None):\n",
    "        \"\"\"\n",
    "        mimics the forward inputs of the EfficientDet PyTorch model.\n",
    "        Args:\n",
    "            x: inputs images\n",
    "            img_info: input image info for scaling the outputs\n",
    "\n",
    "        Returns:\n",
    "            A torch.Tensor of shape [Batch, Boxes, 6], the same as\n",
    "            the PyTorch model\n",
    "\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        keras_input = x.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
    "        outputs = self.model(keras_input)\n",
    "\n",
    "        outs = [torch.Tensor(o.numpy()).to(device) for o in outputs]\n",
    "        # reorder boxes (y, x, y2, x2) to (x, y, x2, y2)\n",
    "        outs[0] = outs[0][:, :, [1, 0, 3, 2]]\n",
    "        # scale boxes to original image size\n",
    "        outs[0] = outs[0] * img_info['img_scale'].view((-1, 1, 1))\n",
    "        return torch.cat([outs[0], outs[1].unsqueeze(2),\n",
    "                          outs[2].unsqueeze(2) + 1], 2)\n",
    "\n",
    "\n",
    "def get_coco_dataloader(batch_size=16, split='val', config=None):\n",
    "    \"\"\"\n",
    "        Get the torch data-loader and evaluation object\n",
    "    Args:\n",
    "        batch_size: batch size for data loader\n",
    "        split: dataset split\n",
    "        config: model config\n",
    "\n",
    "    Returns:\n",
    "        The DataLoader and evaluation object for calculating accuracy\n",
    "\n",
    "    \"\"\"\n",
    "    root = './coco'\n",
    "\n",
    "    args = dict(interpolation='bilinear', mean=None,\n",
    "                std=None, fill_color=None)\n",
    "    dataset = create_dataset('coco', root, split)\n",
    "    input_config = resolve_input_config(args, config)\n",
    "    loader = create_loader(\n",
    "        dataset,\n",
    "        input_size=input_config['input_size'],\n",
    "        batch_size=batch_size,\n",
    "        use_prefetcher=True,\n",
    "        interpolation=input_config['interpolation'],\n",
    "        fill_color=input_config['fill_color'],\n",
    "        mean=input_config['mean'],\n",
    "        std=input_config['std'],\n",
    "        num_workers=0,\n",
    "        pin_mem=False,\n",
    "    )\n",
    "    evaluator = create_evaluator('coco', dataset, pred_yxyx=False)\n",
    "\n",
    "    return loader, evaluator\n",
    "\n",
    "\n",
    "def acc_eval(_model: tf.keras.Model, batch_size=16, config=None):\n",
    "    \"\"\"\n",
    "    This function takes a Keras model, wraps it in a Torch model and runs evaluation\n",
    "    Args:\n",
    "        _model: Keras model\n",
    "        batch_size: batch size of the data loader\n",
    "        config: model config\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # wrap Keras model in a Torch model so it can run in timm's evaluation code\n",
    "    _model = TorchWrapper(_model)\n",
    "    # EValuate input model\n",
    "    val_loader, evaluator = get_coco_dataloader(batch_size=batch_size, config=config)\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    end = time()\n",
    "    last_idx = len(val_loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            output = _model(input, img_info=target)\n",
    "\n",
    "            evaluator.add_predictions(output, target)\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time() - end)\n",
    "            end = time()\n",
    "            if i % 10 == 0 or i == last_idx:\n",
    "                print(\n",
    "                    f'Test: [{i:>4d}/{len(val_loader)}]  '\n",
    "                    f'Time: {batch_time.val:.3f}s ({batch_time.avg:.3f}s, {input.size(0) / batch_time.avg:>7.2f}/s)  '\n",
    "                )\n",
    "\n",
    "    return evaluator.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5833c805a1ca77aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras model\n",
    "\n",
    "Create the Keras model and copy weights from pretrained PyTorch weights file. Saved as \"model.keras\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e589b01c6a45a9e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_name = 'tf_efficientdet_lite0'\n",
    "config = get_efficientdet_config(model_name)\n",
    "\n",
    "model = EfficientDetKeras(config, pretrained_backbone=False).get_model([*config.image_size] + [3])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f1dacee7a949928"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate Keras model\n",
    "\n",
    "We evaluate the model to verify the conversion to a Keras model succeeded. The result will be compared to the quantized model evaluation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ef6b474a69358e03"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "float_map = acc_eval(model, batch_size=64, config=config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc2c87ab3460f395"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantize Keras model\n",
    "\n",
    "In this section, the Keras model will be quantized by the MCT, with the following parameters:\n",
    "- **Target Platform**: IMX500-v1\n",
    "- **Mixed-Precision** weights compression so the model will fit the IMX500 memory size\n",
    "\n",
    "The quantized model is saved as \"quant_model.keras\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aca80a0fc370eef3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "loader, _ = get_coco_dataloader(split='val', config=config)\n",
    "\n",
    "\n",
    "def get_representative_dataset(n_iter):\n",
    "    \"\"\"\n",
    "    This function creates a representative dataset generator\n",
    "    Args:\n",
    "        n_iter: number of iterations for MCT to calibrate on\n",
    "\n",
    "    Returns:\n",
    "        A representative dataset generator\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def representative_dataset():\n",
    "        \"\"\"\n",
    "        Creates a representative dataset generator from a PyTorch data loader, The generator yields numpy\n",
    "        arrays of batches of shape: [Batch, H, W ,C]\n",
    "        Returns:\n",
    "            A representative dataset generator\n",
    "\n",
    "        \"\"\"\n",
    "        ds_iter = iter(loader)\n",
    "        for _ in range(n_iter):\n",
    "            t = next(ds_iter)[0]\n",
    "            # Convert the Torch tensor from the data loader to a numpy array and transpose to the\n",
    "            # right shape: [B, C, H, W] -> [B, H, W, C]\n",
    "            tf_shaped_tensor = t.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
    "            yield [tf_shaped_tensor]\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "\n",
    "# Set IMX500-v1 TPC\n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
    "# set weights memory size, so the quantized model will fit the IMX500 memory\n",
    "resource_utilization = mct.core.ResourceUtilization(weights_memory=2674291)\n",
    "# set MixedPrecision configuration for compressing the weights\n",
    "mp_config = mct.core.MixedPrecisionQuantizationConfig(use_hessian_based_scores=False)\n",
    "core_config = mct.core.CoreConfig(mixed_precision_config=mp_config)\n",
    "quant_model, _ = mct.ptq.keras_post_training_quantization(\n",
    "    model,\n",
    "    get_representative_dataset(20),\n",
    "    target_resource_utilization=resource_utilization,\n",
    "    core_config=core_config,\n",
    "    target_platform_capabilities=tpc)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f1fa147c5a16df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate quantized Keras model\n",
    "\n",
    "Quantized Keras model evaluation applied the same as the original model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79ae299b0b019953"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quant_map = acc_eval(quant_model, batch_size=64, config=config)\n",
    "\n",
    "print(f' ===>> Float model mAP = {100*float_map:2.3f}, Quantized model mAP = {100*quant_map:2.3f}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f93b9b932fb39cc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Export and Load the quantized model\n",
    "Lastly, we will demonstrate how to export the quantized model into a file and then load it.\n",
    "\n",
    "We will use `keras_export_model` function to save the quantized model with the integrated custom quantizers into a \".keras\" file format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee1b78821510df89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Export a keras model with mctq custom quantizers into a file\n",
    "mct.exporter.keras_export_model(model=quant_model,\n",
    "                                save_model_path='./quant_model.keras')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6012dc5634e36841"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we can load the saved model using `keras_load_quantized_model` function. For this specific case, we'll have to supply the load function with an extra custom layer integrated into the model, namely `SSDPostProcess`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a311376344a903"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sony_custom_layers.keras.object_detection.ssd_post_process import SSDPostProcess\n",
    "\n",
    "custom_objects = {SSDPostProcess.__name__: SSDPostProcess} # An extra custom layer integrated in the model \n",
    "quant_model_from_file = mct.keras_load_quantized_model('./quant_model.keras', custom_objects=custom_objects)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c7de31aa90bc002"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\\\n",
    "Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d36d177779d29347"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
