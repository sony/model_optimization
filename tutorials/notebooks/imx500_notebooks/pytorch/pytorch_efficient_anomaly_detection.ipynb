{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGqjJDA1AaJo"
      },
      "source": [
        "# Anomaly Detection Training Benchmark and Quantization for IMX500"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHiBhX_DkkGH"
      },
      "source": [
        "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/imx500_notebooks/pytorch/pytorch_efficient_anomaly_detection.ipynb)\n",
        "\n",
        "### Overview\n",
        "\n",
        "In this tutorial we demonstrate training, quantization and benchmarking of an anomaly detection model. The resulting model will be imx500 compatible"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF92AFYDJGd0"
      },
      "source": [
        "Classification models are powerful and reliable, but what if you have little or no examples of one of your classes, what if one of your classes contains too much unpredictable variation?\n",
        "\n",
        "Here we go through the process of building, training and quantizing an anomaly detection model designed to solve exactly these problems.\n",
        "\n",
        "Anomaly detection models are useful as they only require your typical images to train and can in theory determin anything that is not typical.\n",
        "\n",
        "We use Efficient ad, one of the top performing anomaly detection models on the mvtec benchmark. benchmark leader board can be found [here](https://paperswithcode.com/sota/anomaly-detection-on-mvtec-ad)\n",
        "\n",
        "This particular model uses the teacher student method. Where the student model is trained to both mimic the feature map output of a simple pre-trained CNN aswell as mimic the output of an auto encoder that is its self also trained on the normal images.\n",
        "\n",
        "We use the [mvtec](https://www.mvtec.com/company/research/datasets/mvtec-ad) dataset to benchmark and train this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvXjuRWEnA6f"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this tutorial we will cover for an anomaly detection model:\n",
        "\n",
        "1. Mvtec Benchmark\n",
        "2. Post training quantization.\n",
        "3. Visulization\n",
        "4. Training this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y6W3GB6M3eZ"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### install relevant packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCXBzuGxAFxU",
        "outputId": "d45e5967-9471-47ba-e92d-da32ed116852"
      },
      "outputs": [],
      "source": [
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install tifffile\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install Pillow\n",
        "!pip install scipy\n",
        "!pip install tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install MCT (if it’s not already installed). Additionally, in order to use all the necessary utility functions for this tutorial, we also copy [MCT tutorials folder](https://github.com/sony/model_optimization/tree/main/tutorials) and add it to the system path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import importlib\n",
        "\n",
        "if not importlib.util.find_spec('model_compression_toolkit'):\n",
        "    !pip install model_compression_toolkit\n",
        "!git clone https://github.com/sony/model_optimization.git temp_mct && mv temp_mct/tutorials . && \\rm -rf temp_mct\n",
        "sys.path.insert(0,\"tutorials\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGqXJH15oLvm"
      },
      "source": [
        "Download and extract the mvtec benchmark dataset. This is used for both training and evaluation. Link below is a direct link from the mcvtec website. \n",
        "\n",
        "For more information on the Mvtec Benchmark dataset, visit: https://www.mvtec.com/company/research/datasets/mvtec-ad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOqdM0peKsZ9",
        "outputId": "2c8c5730-71a8-4fe9-8108-046a24780903"
      },
      "outputs": [],
      "source": [
        "!mkdir mvtec_anomaly_detection\n",
        "!wget https://www.mydrive.ch/shares/38536/3830184030e49fe74747669442f0f282/download/420938113-1629952094/mvtec_anomaly_detection.tar.xz\n",
        "!tar -xvf mvtec_anomaly_detection.tar.xz -C mvtec_anomaly_detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX_WMo1zomnk"
      },
      "source": [
        "Finally download the official mvtec benchmark. Link below is a direct link from the mcvtec website."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTRG_ebOZQZJ",
        "outputId": "b8218f5a-856c-4969-84b5-a6f09f67c8f4"
      },
      "outputs": [],
      "source": [
        "!wget https://www.mydrive.ch/shares/60736/698155e0e6d0467c4ff6203b16a31dc9/download/439517473-1665667812/mvtec_ad_evaluation.tar.xz\n",
        "!tar -xvf mvtec_ad_evaluation.tar.xz\n",
        "!rm mvtec_ad_evaluation.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nvWU5--PJy0"
      },
      "source": [
        "## Model Quantization\n",
        "\n",
        "### Download and Build Model\n",
        "\n",
        "We have pretrained a model on the bottle dataset from mvtec. Here we will load the combined model from huggingface, a combination of (teacher, student, and autoenoder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201,
          "referenced_widgets": [
            "0a490fcaf8e44211a7138ff84d617788",
            "a95f884036e648739e1c3b7d87269219",
            "dddd26a508d64fd4bec206b4a05eb382",
            "756e67b40ca44fec9f2941aaa547a130",
            "34f0d6010c3b4eba83efd1a113e6312d",
            "ac6f54e6ee604cd99208674531fcca17",
            "3cbc9af0fb2148edb893e369649c2edc",
            "7618819d67624f4ead9cd06fdedb5de5",
            "5dd006f38b314a88b573a79e4d4955fc",
            "5d8a2e15ec8c47158ede018efe0d8353",
            "c7723d0bb97b4a1a9c5bd4434e6a17ff"
          ]
        },
        "id": "PuTz0cryyRD6",
        "outputId": "dd922691-e229-410d-bea9-35977f4b6279"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import tutorials.mct_model_garden.models_pytorch.Efficient_Anomaly_Det as efficient_ad\n",
        "\n",
        "out_channels = 384\n",
        "\n",
        "model_path = hf_hub_download(repo_id=\"SSI-DNN/Efficient_Anomaly_Detection\", filename=\"efficientAD_bottle.pth\")\n",
        "\n",
        "teacher = efficient_ad.get_pdn_small(out_channels)\n",
        "student = efficient_ad.get_pdn_small(2 * out_channels)\n",
        "autoencoder = efficient_ad.get_autoencoder(out_channels)\n",
        "\n",
        "model = efficient_ad.UnifiedAnomalyDetectionModel.load_model(model_path, teacher, student, autoencoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCOLpRKLqi6Z"
      },
      "source": [
        "### Post training quantization using Model Compression Toolkit\n",
        "\n",
        "To perform model quantization we require a representative dataset. Because of the specific requirements of anomaly detection models (the assumption that anomalous images are not seen) we restrict the representative dataset to as such."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "481cEIZSa0bl",
        "outputId": "a77f69b4-341d-498d-f559-8f7978fab0d9"
      },
      "outputs": [],
      "source": [
        "import model_compression_toolkit as mct\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from typing import Iterator, List\n",
        "from tutorials.resources.utils.efficient_ad_utils import ImageFolderWithoutTarget, train_transform\n",
        "\n",
        "\n",
        "def train_dataset_generator(train_loader: DataLoader) -> Iterator[List]:\n",
        "    while True:\n",
        "        for data, _ in train_loader:\n",
        "            yield [data.numpy()]\n",
        "\n",
        "def get_representative_dataset(n_iter: int, dataset_loader: Iterator[List]) -> Iterator[List]:\n",
        "    def representative_dataset() -> Iterator[List]:\n",
        "        ds_iter = iter(dataset_loader)\n",
        "        for _ in range(n_iter):\n",
        "            yield next(ds_iter)\n",
        "    return representative_dataset\n",
        "\n",
        "train_set = ImageFolderWithoutTarget(\n",
        "    os.path.join('./mvtec_anomaly_detection', 'bottle', 'train'),\n",
        "    transform=transforms.Lambda(train_transform))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)  # Ensure this matches your batch size and other DataLoader settings\n",
        "train_dataset = train_dataset_generator(train_loader)\n",
        "representative_dataset_gen = get_representative_dataset(n_iter=20, dataset_loader=train_dataset)\n",
        "\n",
        "# Set target platform capabilities\n",
        "tpc = mct.get_target_platform_capabilities(fw_name=\"pytorch\", target_platform_name='imx500', target_platform_version='v1')\n",
        "\n",
        "# Perform post training quantization\n",
        "quant_model, _ = mct.ptq.pytorch_post_training_quantization(in_module=model,\n",
        "                                                            representative_data_gen=representative_dataset_gen,\n",
        "                                                            target_platform_capabilities=tpc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZXaCUenskKG"
      },
      "source": [
        "## Model Export\n",
        "\n",
        "This model can be converted to run on imx500.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrKrsmSasm34"
      },
      "outputs": [],
      "source": [
        "import model_compression_toolkit as mct\n",
        "\n",
        "mct.exporter.pytorch_export_model(model=quant_model,\n",
        "                                  save_model_path='./quant_model.onnx',\n",
        "                                  repr_dataset=representative_dataset_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZXDd4bXA_4E"
      },
      "source": [
        "## Float Benchmark\n",
        "\n",
        "Mvtec benchmark provides its own code. we first need to run the model on the test images and save the output heat maps and anomaly confidence. then run mvtecs benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#We first need to calculate the normalisation values\n",
        "\n",
        "q_st_start, q_st_end, q_ae_start, q_ae_end = map_normalization(\n",
        "    validation_loader=train_loader, teacher=teacher, student=student,\n",
        "    autoencoder=autoencoder, teacher_mean=teacher_mean,\n",
        "    teacher_std=teacher_std, desc='Final map normalization')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yP2FGLY8BMlV",
        "outputId": "0092a064-6893-41b1-f084-1afd8c807333"
      },
      "outputs": [],
      "source": [
        "from tutorials.mct_model_garden.anomaly_eval import benchmark\n",
        "benchmark(model, 'mvtec_ad', q_st_start, q_st_end, q_ae_start, q_ae_end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITfY-C6ylrUi"
      },
      "source": [
        "### Mvtec benchmark\n",
        "\n",
        "This results in a classification accuracy AU-ROC and a segmentation accuracy AU-PRO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KeelewrtZdUA",
        "outputId": "7b2cf763-29a5-4b2a-fadd-b18347a05d6f"
      },
      "outputs": [],
      "source": [
        "!python ./mvtec_ad_evaluation/evaluate_experiment.py --dataset_base_dir './mvtec_anomaly_detection/' --anomaly_maps_dir './output/anomaly_maps/mvtec_ad/' --output_dir './output/metrics/mvtec_ad/' --evaluated_objects bottle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fPTNtJLgPc5"
      },
      "source": [
        "## Quantized model benchmark\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siM716G2506g",
        "outputId": "3e5253b7-c3ac-408d-fd15-3e586744523d"
      },
      "outputs": [],
      "source": [
        "from tutorials.mct_model_garden.anomaly_eval import benchmark\n",
        "benchmark(quant_model, 'mvtec_ad_quant')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVbfzRVv6YJC",
        "outputId": "9d5cb7da-d239-47cd-d019-3de2921f5d60"
      },
      "outputs": [],
      "source": [
        "!python ./mvtec_ad_evaluation/evaluate_experiment.py --dataset_base_dir './mvtec_anomaly_detection/' --anomaly_maps_dir './output/anomaly_maps/mvtec_ad_quant/' --output_dir './output/metrics/mvtec_ad/' --evaluated_objects bottle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xD2E4k0XibZ_"
      },
      "source": [
        "## Anomaly Map Visulization\n",
        "\n",
        "We can visulize the heatmap of the predicted anomalies with the code below. Here red spots indicate locations with a high likely hood of defect, based on its training images. We also print the models prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SfK27g5J95Kd",
        "outputId": "3266c83d-2ea6-4b1b-ea15-290053553a67"
      },
      "outputs": [],
      "source": [
        "from tutorials.resources.utils.efficient_ad_utils import visualize_anomalies\n",
        "import os\n",
        "name = 'visulize'\n",
        "dataset_path = './mvtec_anomaly_detection'\n",
        "test_output_dir = os.path.join('output', 'anomaly_maps',\n",
        "                                name, 'bottle', 'test')\n",
        "model.eval()\n",
        "visualize_anomalies(model, dataset_path, test_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1zwMJyri8by"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook we provide examples on how to quantize and benchmark the latest anomaly detection model as well as providing code to visulize the models output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\\\n",
        "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a490fcaf8e44211a7138ff84d617788": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a95f884036e648739e1c3b7d87269219",
              "IPY_MODEL_dddd26a508d64fd4bec206b4a05eb382",
              "IPY_MODEL_756e67b40ca44fec9f2941aaa547a130"
            ],
            "layout": "IPY_MODEL_34f0d6010c3b4eba83efd1a113e6312d"
          }
        },
        "34f0d6010c3b4eba83efd1a113e6312d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cbc9af0fb2148edb893e369649c2edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d8a2e15ec8c47158ede018efe0d8353": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd006f38b314a88b573a79e4d4955fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "756e67b40ca44fec9f2941aaa547a130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d8a2e15ec8c47158ede018efe0d8353",
            "placeholder": "​",
            "style": "IPY_MODEL_c7723d0bb97b4a1a9c5bd4434e6a17ff",
            "value": " 32.3M/32.3M [00:02&lt;00:00, 13.9MB/s]"
          }
        },
        "7618819d67624f4ead9cd06fdedb5de5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a95f884036e648739e1c3b7d87269219": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac6f54e6ee604cd99208674531fcca17",
            "placeholder": "​",
            "style": "IPY_MODEL_3cbc9af0fb2148edb893e369649c2edc",
            "value": "efficientAD_bottle.pth: 100%"
          }
        },
        "ac6f54e6ee604cd99208674531fcca17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7723d0bb97b4a1a9c5bd4434e6a17ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dddd26a508d64fd4bec206b4a05eb382": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7618819d67624f4ead9cd06fdedb5de5",
            "max": 32265074,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5dd006f38b314a88b573a79e4d4955fc",
            "value": 32265074
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
