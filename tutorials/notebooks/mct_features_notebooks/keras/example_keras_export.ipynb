{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Export a Quantized Keras Model With the Model Compression Toolkit (MCT)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_export.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates how to export a Keras model to `.keras` and TFLite formats using the Model Compression Toolkit (MCT). It covers the steps of creating a simple Keras model, applying post-training quantization (PTQ) using MCT, and then exporting the quantized model to `.keras` and TFLite. The tutorial also shows how to use the exported model for inference.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Constructing a simple Keras model for demonstration purposes.\n",
    "2. Applying post-training quantization to the model using the Model Compression Toolkit.\n",
    "3. Exporting the quantized model to the `.keras` and `TFLite` formats.\n",
    "4. Using the exported model for inference.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ],
   "metadata": {
    "id": "UJDzewEYfSN5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TF_VER = '2.14.0'\n",
    "\n",
    "!pip install -q tensorflow=={TF_VER}"
   ],
   "metadata": {
    "id": "qNddNV6TEsX0"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "\n",
    "float_model = MobileNetV2()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Quantize the Model with the Model Compression Toolkit\n",
    "Let's begin by applying quantization using MCT. This process will prepare the model for export.\n",
    "\n",
    "### Representative Dataset\n",
    "For post-training quantization with MCT, a representative dataset is required.  In this example, we use a random dataset for demonstration purposes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Quantize the model.\n",
    "# Notice that here the representative dataset is random for demonstration only.\n",
    "quantized_exportable_model, _ = mct.ptq.keras_post_training_quantization(float_model,\n",
    "                                                                         representative_data_gen=lambda: [np.random.random((1, 224, 224, 3))])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Keras export\n",
    "The model will be exported as a tensorflow `.keras` model, where both weights and activations are represented as dtype float32.\n",
    "There are two optional formats available for export: MCTQ and FAKELY_QUANT.\n",
    "\n",
    "#### MCTQ\n",
    "\n",
    "By default, `mct.exporter.keras_export_model` exports the quantized Keras model to a `.keras` model using custom quantizers from the mct_quantizers module. "
   ],
   "metadata": {
    "id": "-n70LVe6DQPw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path of exported model\n",
    "keras_file_path = 'exported_model_mctq.keras'\n",
    "\n",
    "# Export a keras model with mctq custom quantizers.\n",
    "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
    "                                save_model_path=keras_file_path)"
   ],
   "metadata": {
    "id": "PO-Hh0bzD1VJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the model's size remains unchanged compared to the quantized exportable model, as the weight data types are still represented as floats.\n",
    "#### MCTQ - Loading the Exported Model\n",
    "\n",
    "To load the exported model with MCTQ quantizers, use `mct.keras_load_quantized_model`:"
   ],
   "metadata": {
    "id": "Bwx5rxXDF_gb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loaded_model = mct.keras_load_quantized_model(keras_file_path)"
   ],
   "metadata": {
    "id": "q235XNJQmTdd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fakely-Quantized Format\n",
    "To export a fakely-quantized model, use the `QuantizationFormat.FAKELY_QUANT` option. This format ensures that quantization is simulated but does not alter the data types of the weights and activations during export."
   ],
   "metadata": {
    "id": "sOmDjSehlQba"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path of exported model\n",
    "keras_file_path = 'exported_model_fakequant.keras'\n",
    "\n",
    "# Use mode KerasExportSerializationFormat.KERAS for a .keras model\n",
    "# and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights\n",
    "# and activations.\n",
    "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
    "                                save_model_path=keras_file_path,\n",
    "                                quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
   ],
   "metadata": {
    "id": "WLyHEEiwGByT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the fakely-quantized model has the same size as the quantized exportable model, as the weights are still represented as floats.\n",
    "\n",
    "### TFLite\n",
    "There are two optional tflite serializations available for export: `INT8` and `FAKELY_QUANT`.\n",
    "\n",
    "#### INT8 TFLite\n",
    "\n",
    "The model will be exported as a tflite model where weights and activations are represented as 8bit integers."
   ],
   "metadata": {
    "id": "-L1aRxFGGFeF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "tflite_file_path = 'exported_model_int8.tflite'\n",
    "\n",
    "# Use mode KerasExportSerializationFormat.TFLITE for tflite model and quantization_format.INT8.\n",
    "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
    "                                save_model_path=tflite_file_path,\n",
    "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
    "                                quantization_format=mct.exporter.QuantizationFormat.INT8)"
   ],
   "metadata": {
    "id": "V4I-p1q5GLzs"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Compare size of float and quantized model:\n"
   ],
   "metadata": {
    "id": "SBqtJV9AGRzN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Save float model to measure its size\n",
    "float_file_path = 'exported_model_float.keras'\n",
    "float_model.save(float_file_path)\n",
    "\n",
    "print(\"Float model in Mb:\", os.path.getsize(float_file_path) / float(2 ** 20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(tflite_file_path) / float(2 ** 20))"
   ],
   "metadata": {
    "id": "LInM16OMGUtF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Fakely-Quantized TFLite\n",
    "\n",
    "The model will be exported as a tflite model where weights and activations are quantized but represented with a float data type."
   ],
   "metadata": {
    "id": "9eVDoIHiGX5-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Path of exported model\n",
    "tflite_file_path = 'exported_model_fakequant.tflite'\n",
    "\n",
    "\n",
    "# Use mode KerasExportSerializationFormat.TFLITE for tflite model and QuantizationFormat.FAKELY_QUANT for fakely-quantized weights\n",
    "# and activations.\n",
    "mct.exporter.keras_export_model(model=quantized_exportable_model,\n",
    "                                save_model_path=tflite_file_path,\n",
    "                                serialization_format=mct.exporter.KerasExportSerializationFormat.TFLITE,\n",
    "                                quantization_format=mct.exporter.QuantizationFormat.FAKELY_QUANT)"
   ],
   "metadata": {
    "id": "0OYLAbI8Gawu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that the fakely-quantized model has the same size as the quantized exportable model, as the weights are still represented as floats."
   ],
   "metadata": {
    "id": "voOrtCroD-HE"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ]
}
