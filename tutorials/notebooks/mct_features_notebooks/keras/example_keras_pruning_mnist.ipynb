{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Structured Pruning of a Fully-Connected Keras Model using the Model Compression Toolkit (MCT)\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/keras/example_keras_pruning_mnist.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial provides a step-by-step guide to training, pruning, and finetuning a Keras fully connected neural network model using the Model Compression Toolkit (MCT). We will start by building and training the model from scratch on the MNIST dataset, followed by applying structured pruning to reduce the model size.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. **Training a Keras model on MNIST:** We'll begin by constructing a basic fully connected neural network and training it on the MNIST dataset. \n",
    "2. **Applying structured pruning:** We'll introduce a pruning technique to reduce model size while maintaining performance. \n",
    "3. **Finetuning the pruned model:** After pruning, we'll finetune the model to recover any lost accuracy. \n",
    "4. **Evaluating the pruned model:** We'll evaluate the pruned model’s performance and compare it to the original model.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ],
   "metadata": {
    "id": "UJDzewEYfSN5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "TF_VER = '2.14.0'\n",
    "!pip install -q tensorflow[and-cuda]~={TF_VER}"
   ],
   "metadata": {
    "id": "xTvVA__4NItc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q2bAksKtM0ca"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import model_compression_toolkit as mct\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading and Preprocessing MNIST\n",
    "Let's define the dataset loaders to retrieve the train and test parts of the MNIST dataset, including preprocessing:"
   ],
   "metadata": {
    "id": "tW1xcK_Kf4F_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the MNIST dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the images to [0, 1] range\n",
    "train_images = train_images.astype('float32') / 255.0\n",
    "test_images = test_images.astype('float32') / 255.0\n"
   ],
   "metadata": {
    "id": "fwtJHnflfv_f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating a Fully-Connected Model\n",
    "In this section, we create a simple example of a fully connected model to demonstrate the pruning process. It consists of three dense layers with 128, 64, and 10 neurons. After defining the model architecture, we compile it to prepare for training and evaluation."
   ],
   "metadata": {
    "id": "m3vu7-uvgtfC"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def create_model():\n",
    "    model = tf.keras.models.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return model"
   ],
   "metadata": {
    "id": "If3oj5jSjXen"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Dense Model on MNIST\n",
    "Next, we will train the dense model using the preprocessed MNIST dataset."
   ],
   "metadata": {
    "id": "Q_tK6Xknbtha"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Train and evaluate the model\n",
    "model = create_model()\n",
    "model.fit(train_images, train_labels, epochs=6, validation_data=(test_images, test_labels))\n",
    "model.evaluate(test_images, test_labels)"
   ],
   "metadata": {
    "id": "jQ3_9Z1WllVV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dense Model Properties\n",
    "The `model.summary()` function in Keras provides a comprehensive overview of the model's architecture, including each layer's type, output shapes, and the number of trainable parameters."
   ],
   "metadata": {
    "id": "ZQHxLrsvcLKH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.summary()"
   ],
   "metadata": {
    "id": "oxdespw2eeBW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's break down the details from our model summary:\n",
    "\n",
    "- **First Dense Layer:** A fully connected layer with 128 output channels and 784 input channels.\n",
    "- **Second Dense Layer:** A fully connected layer with 64 output channels and 128 input channels.\n",
    "- **Third Dense Layer:** The final layer with 10 neurons (matching the number of MNIST classes) and 64 input channels.\n",
    "\n",
    "The model has a total of 109,386 parameters, requiring approximately 427.29 KB of memory."
   ],
   "metadata": {
    "id": "GymibwxQehOL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MCT Structured Pruning\n",
    "\n",
    "### Target Platform Capabilities (TPC)\n",
    "MCT optimizes models for dedicated hardware using Target Platform Capabilities (TPC). For more details, please refer to our [documentation](https://sony.github.io/model_optimization/docs/api/api_docs/modules/target_platform.html)). First, we'll configure the TPC to define each layer's SIMD (Single Instruction, Multiple Data) size.\n",
    "\n",
    "In MCT, SIMD plays a key role in channel grouping, influencing the pruning process by considering channel importance within each SIMD group.\n",
    "\n",
    "For this demonstration, we'll use the simplest structured pruning scenario with SIMD set to 1."
   ],
   "metadata": {
    "id": "RKatTp55emtF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from model_compression_toolkit.target_platform_capabilities.target_platform import Signedness\n",
    "tp = mct.target_platform\n",
    "\n",
    "simd_size = 1\n",
    "\n",
    "def get_tpc():\n",
    "    # Define the default weight attribute configuration\n",
    "    default_weight_attr_config = tp.AttributeQuantizationConfig(\n",
    "        weights_quantization_method=tp.QuantizationMethod.UNIFORM,\n",
    "    )\n",
    "\n",
    "    # Define the OpQuantizationConfig\n",
    "    default_config = tp.OpQuantizationConfig(\n",
    "        default_weight_attr_config=default_weight_attr_config,\n",
    "        attr_weights_configs_mapping={},\n",
    "        activation_quantization_method=tp.QuantizationMethod.UNIFORM,\n",
    "        activation_n_bits=8,\n",
    "        supported_input_activation_n_bits=8,\n",
    "        enable_activation_quantization=None,\n",
    "        quantization_preserving=None,\n",
    "        fixed_scale=None,\n",
    "        fixed_zero_point=None,\n",
    "        simd_size=simd_size,\n",
    "        signedness=Signedness.AUTO\n",
    "    )\n",
    "\n",
    "    # Create the quantization configuration options and model\n",
    "    default_configuration_options = tp.QuantizationConfigOptions([default_config])\n",
    "    tp_model = tp.TargetPlatformModel(default_configuration_options)\n",
    "\n",
    "    # Return the target platform capabilities\n",
    "    tpc = tp.TargetPlatformCapabilities(tp_model)\n",
    "    return tpc\n"
   ],
   "metadata": {
    "id": "wqZ71s70jXhH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Representative Dataset\n",
    "We are creating a representative dataset to guide the model pruning process. It is used to compute an importance score for each channel. This dataset is implemented as a generator that returns a list of images."
   ],
   "metadata": {
    "id": "SnKxedEgqdSm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "def representative_data_gen():\n",
    "  indices = random.sample(range(len(train_images)), 32)\n",
    "  yield [np.stack([train_images[i] for i in indices])]"
   ],
   "metadata": {
    "id": "SCiXV1s9jswp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resource Utilization\n",
    "We define a `resource_utilization` limit to constrain the memory usage of the pruned model. We'll prune our trained model to reduce its size, aiming for a 50% reduction in the memory footprint of the model's weights. Since the weights use the float32 data type (each parameter occupying 4 bytes), we calculate the memory usage by multiplying the total number of parameters by 4. By setting a target to limit the model's weight memory to around 214 KB, we aim for a 50% compression ratio."
   ],
   "metadata": {
    "id": "nylQtALnr9gN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Create a ResourceUtilization object to limit the pruned model weights memory to a certain resource constraint\n",
    "dense_model_memory = 427*(2**10) # Original model weights requiers ~427KB\n",
    "compression_ratio = 0.5\n",
    "\n",
    "resource_utilization = mct.core.ResourceUtilization(weights_memory=dense_model_memory*compression_ratio)"
   ],
   "metadata": {
    "id": "doJgwbSxsCbr"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model Pruning\n",
    "We are now ready to perform the actual pruning using MCT’s `keras_pruning_experimental` function. The model will be pruned based on the defined resource utilization constraints and the previously generated representative dataset.\n",
    "\n",
    "Each channel’s importance is measured using the [LFH (Label-Free-Hessian) method](https://arxiv.org/abs/2309.11531), which approximates the Hessian of the loss function with respect to the model’s weights.\n",
    "\n",
    "For efficiency, we use a single score approximation. Although less precise, it significantly reduces processing time compared to multiple approximations, which offer better accuracy but at the cost of longer runtimes.\n",
    "\n",
    "MCT’s structured pruning will target the first two dense layers, where output channel reduction can be propagated to subsequent layers by adjusting their input channels accordingly.\n",
    "\n",
    "The output is a pruned model along with pruning information, including layer-specific pruning masks and scores."
   ],
   "metadata": {
    "id": "xSP6815rsCnc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "num_score_approximations = 1\n",
    "\n",
    "target_platform_cap = get_tpc()\n",
    "pruned_model, pruning_info = mct.pruning.keras_pruning_experimental(\n",
    "        model=model,\n",
    "        target_resource_utilization=resource_utilization,\n",
    "        representative_data_gen=representative_data_gen,\n",
    "        target_platform_capabilities=target_platform_cap,\n",
    "        pruning_config=mct.pruning.PruningConfig(num_score_approximations=num_score_approximations)\n",
    "    )"
   ],
   "metadata": {
    "id": "x4taG-5TxBrp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pruned Model Properties\n",
    "As before, we can use the Keras model API to inspect the new architecture and details of the pruned model."
   ],
   "metadata": {
    "id": "iPd6ezZN2DNp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pruned_model.summary()"
   ],
   "metadata": {
    "id": "xZu4gPwz2Ptp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Finetuning the Pruned Model\n",
    "After pruning, it’s common to see a temporary drop in model accuracy due to the reduction in model complexity. Let’s demonstrate this by evaluating the pruned model and observing its initial performance before finetuning."
   ],
   "metadata": {
    "id": "pAheQ9SGxB13"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pruned_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "pruned_model.evaluate(test_images, test_labels)"
   ],
   "metadata": {
    "id": "Vpihq5fpdeSA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To restore the model's performance, we finetune the pruned model, allowing it to adapt to its new, compressed architecture. Through this finetuning process, the model can often recover its original accuracy, and in some cases, even surpass it."
   ],
   "metadata": {
    "id": "IHORL34t17bA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pruned_model.fit(train_images, train_labels, epochs=6, validation_data=(test_images, test_labels))\n",
    "pruned_model.evaluate(test_images, test_labels)"
   ],
   "metadata": {
    "id": "q00zV9Jmjszo"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "In this tutorial, we explored the process of structured model pruning using MCT to optimize a dense neural network. We demonstrated how to define resource constraints, apply pruning based on channel importance, and evaluate the impact on model architecture and performance. Finally, we showed how finetuning can recover the pruned model’s accuracy. This approach highlights the effectiveness of structured pruning for reducing model size while maintaining performance, making it a powerful tool for model optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "Copyright 2023 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ]
}
