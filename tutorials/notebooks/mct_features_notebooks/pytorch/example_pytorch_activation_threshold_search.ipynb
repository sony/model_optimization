{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# A Practical Guide to Activation Threshold Search in Post-Training Quantization\n",
    "\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_activation_threshold_search.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates how to find the optimal activation threshold, a key component in MCT's post-training quantization workflow.\n",
    "\n",
    "In this example, we will explore two different metrics for threshold selection. We will begin by applying the appropriate MCT configurations, followed by inferring a representative dataset through the model. Next, we will plot the activation distributions of two layers along with their corresponding MCT-calculated thresholds, and finally, we will compare the quantized model accuracy using both methods.\n",
    "\n",
    "## Activation threshold explanation\n",
    "During the quantization process, thresholds are used to map a distribution of 32-bit floating-point values to their quantized equivalents. Minimizing data loss while preserving the most representative range is crucial for maintaining the final model's accuracy.\n",
    "\n",
    "### How Is It Done in MCT?\n",
    "\n",
    "MCT's post-training quantization leverages a representative dataset to evaluate a range of typical output activation values. The challenge lies in determining the best way to map these values to their quantized versions. To address this, a grid search is performed to find the optimal threshold using various error metrics. Typically, mean squared error (MSE) is the most effective and is used as the default metric.\n",
    "\n",
    "The error is calculated based on the difference between the original float and the quantized distributions. The optimal threshold is then selected based on the metric that results in the minimum error. For example, for the case of MSE.\n",
    "\n",
    "$$\n",
    "ERR(t) = \\frac{1}{n_s} \\sum_{X \\in Fl(D)} (Q(X, t, n_b) - X)^2\n",
    "$$\n",
    "\n",
    "- $ERR(t)$ : The quantization error function dependent on the threshold $t$.\n",
    "\n",
    "- $n_s$: The size of the representative dataset.\n",
    "\n",
    "- $\\sum$: Summation over all elements $X$ in the flattened dataset $F_l(D)$.\n",
    "\n",
    "- $F_l(D)$: The set of activation tensors in the $l$-th layer, flattened for processing.\n",
    "\n",
    "- $Q(X, t, n_b)$: The quantized approximation of $X$, given a threshold $t$ and bit width $n_b$.\n",
    "\n",
    "- $X$: The original activation tensor before quantization.\n",
    "\n",
    "- $t$: The quantization threshold, a key parameter for controlling the quantization process.\n",
    "\n",
    "- $n_b$: The number of bits used in quantization, impacting model precision and size.\n",
    "\n",
    "\n",
    "Quantization thresholds often have specific limitations, typically imposed for deployment purposes. In MCT, activation thresholds are restricted by default to Power-of-Two values and can represent either signed values within the range $(-T, T)$ or unsigned values within $(0, T)$. Other restriction settings are also configurable.\n",
    "\n",
    "### Error methods supported by MCT:\n",
    "\n",
    "- **NOCLIPPING:** Use min/max values as thresholds.\n",
    "\n",
    "- **MSE:** Minimizes quantization noise by using the mean squared error (MSE).\n",
    "\n",
    "- **MAE:** Minimizes quantization noise by using the mean absolute error (MAE).\n",
    "\n",
    "- **KL:** Uses Kullback-Leibler (KL) divergence to align the distributions, ensuring that the quantized distribution is as similar as possible to the original.\n",
    "\n",
    "- **Lp:** Minimizes quantization noise using the Lp norm, where `p` is a configurable parameter that determines the type of distance metric.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7837babf2112542b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d691159f5bfc53e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load a pre-trained MobileNetV2 model from pytorch, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "468d67cd5f25886e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5a1be0c4fc4847",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Dataset preparation\n",
    "### Download the ImageNet validation set\n",
    "Download the ImageNet dataset with only the validation split.\n",
    "**Note:** For demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "_ztv72uM6-UT",
   "metadata": {
    "id": "_ztv72uM6-UT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d878e0",
   "metadata": {},
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e5658c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa0bca3e15fba91",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bdb4144e4ce2ab6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_iter = 10\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f4bbca00996989",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Target Platform Capabilities\n",
    "MCT optimizes the model for dedicated hardware. This is done using TPC (for more details, please visit our [documentation](https://sony.github.io/model_optimization/api/api_docs/modules/target_platform.html)). Here, we use the default Tensorflow TPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554719effaf90250",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a TargetPlatformCapabilities object that models the hardware platform for the quantized model inference. Here, for example, we use the default platform that is attached to a Pytorch layers representation.\n",
    "target_platform_cap = mct.get_target_platform_capabilities('pytorch', 'default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
   },
   "source": [
    "## Post-Training Quantization using MCT\n",
    "In this step, we load the model and apply post-training quantization using two threshold error calculation methods: **\"No Clipping\"** and **MSE**.\n",
    "\n",
    "- **\"No Clipping\"** selects the lowest power-of-two threshold that ensures no data is lost (clipped).\n",
    "- **MSE** selects a power-of-two threshold that minimizes the mean square error between the original float distribution and the quantized distribution.\n",
    "\n",
    "- As a result, the \"No Clipping\" method typically results in a larger threshold, as we will demonstrate later in this tutorial.\n",
    "\n",
    "The quantization parameters are predefined, and we use the default values except for the quantization method. Feel free to modify the code below to experiment with other error metrics supported by MCT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "jtiZzXmTjxuI",
   "metadata": {
    "id": "jtiZzXmTjxuI"
   },
   "outputs": [],
   "source": [
    "from model_compression_toolkit.core import QuantizationErrorMethod\n",
    "\n",
    "q_configs_dict = {}\n",
    "# Error methods to iterate over\n",
    "error_methods = [\n",
    "    QuantizationErrorMethod.MSE,\n",
    "    QuantizationErrorMethod.NOCLIPPING\n",
    "]\n",
    "\n",
    "# If you are curious you can add any of the below quantization methods as well.\n",
    "# QuantizationErrorMethod.MAE\n",
    "# QuantizationErrorMethod.KL\n",
    "# QuantizationErrorMethod.LP\n",
    "\n",
    "# Iterate and build the QuantizationConfig objects\n",
    "for error_method in error_methods:\n",
    "    q_config = mct.core.QuantizationConfig(\n",
    "        activation_error_method=error_method,\n",
    "    )\n",
    "\n",
    "    q_configs_dict[error_method] = q_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8W3Dcn0jkJOH",
   "metadata": {
    "id": "8W3Dcn0jkJOH"
   },
   "source": [
    "Now we will run post-training quantization for each configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c6e55-d474-4dc3-9a43-44b736635998",
   "metadata": {
    "id": "ba0c6e55-d474-4dc3-9a43-44b736635998"
   },
   "outputs": [],
   "source": [
    "quantized_models_dict = {}\n",
    "\n",
    "for error_method, q_config in q_configs_dict.items():\n",
    "    # Create a CoreConfig object with the current quantization configuration\n",
    "    ptq_config = mct.core.CoreConfig(quantization_config=q_config)\n",
    "\n",
    "    # Perform MCT post-training quantization\n",
    "    quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_dataset_gen,\n",
    "        core_config=ptq_config,\n",
    "        target_platform_capabilities=target_platform_cap\n",
    "    )\n",
    "\n",
    "    # Update the dictionary to include the quantized model\n",
    "    quantized_models_dict[error_method] = {\n",
    "        \"quantization_config\": q_config,\n",
    "        \"quantized_model\": quantized_model,\n",
    "        \"quantization_info\": quantization_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8UHRsh2khM4",
   "metadata": {
    "id": "A8UHRsh2khM4"
   },
   "source": [
    "## Threshold and Distribution Visualization\n",
    "To facilitate understanding, we will plot the activation distributions for two layers of MobileNetV2. For each layer, we will show the thresholds determined by both **MSE** and **No Clipping** methods, along with the corresponding activation distributions obtained by infering the representative dataset through the model. This visualization highlights the trade-off between data loss and data resolution under different thresholds during quantization.\n",
    "\n",
    "MCT’s `quantization_info` stores the threshold values for each layer. However, to view the actual activation distributions, the model needs to be reconstructed up to and including the target layer selected for visualization.\n",
    "\n",
    "To do this, we first need to identify the layer names. In Keras, this can be easily done for the first 10 layers using the following code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac",
   "metadata": {
    "id": "a22e6d68-c40f-40bf-ab74-ff453011aeac"
   },
   "outputs": [],
   "source": [
    "for index, (name, layer) in enumerate(float_model.named_modules()):\n",
    "    if index < 10:\n",
    "        print(name, layer)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277",
   "metadata": {
    "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277"
   },
   "source": [
    "The first activation layer in the model is named `ReLU6`.\n",
    "\n",
    "For this particular model, testing has shown that the `BatchNorm2d` layer exhibits different thresholds for the two error metrics. Therefore, we will also include this layer in the visualization. For context, MobileNetV2 uses an inverted residual structure, where the input is first expanded in the channel dimension, then passed through a depthwise convolution, and finally projected back to a lower dimension. The `BatchNorm2d` layer represents this projection, and the BN suffix indicates the presence of Batch Normalization.\n",
    "\n",
    "We will use these layer names to create two separate models, each ending at one of these respective layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d",
   "metadata": {
    "id": "1f9dd3f3-6e22-4be9-9beb-29568ff14c9d"
   },
   "outputs": [],
   "source": [
    "layer_name1 = 'features.0.2'            # Conv1_relu\n",
    "layer_name2 = 'features.1.conv.1'       # expanded_conv_project_BN\n",
    "\n",
    "# add hook\n",
    "def get_layer_output(model, layer_name):\n",
    "    outputs = {}\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        outputs[layer_name] = output\n",
    "\n",
    "    # add hook\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# get Conv1_relu's output\n",
    "output_dict_relu = get_layer_output(float_model, layer_name1)\n",
    "\n",
    "# get expanded_conv_project_BN's output（features.1.conv.1 is next to activation-layer）\n",
    "output_dict_project = get_layer_output(float_model, layer_name2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364",
   "metadata": {
    "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364"
   },
   "source": [
    "Infer the representative dataset using these models and store the outputs for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bf3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "activation_batches_relu = []\n",
    "activation_batches_project = []\n",
    "with torch.no_grad():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    float_model = float_model.to(device)\n",
    "    for data in tqdm(representative_dataset_gen()):\n",
    "        images = data[0]\n",
    "        images = images.to(device)\n",
    "\n",
    "        float_model(images)\n",
    "\n",
    "        activations_relu = output_dict_relu[layer_name1]\n",
    "        activation_batches_relu.append(activations_relu.to('cpu').detach().numpy().copy())\n",
    "        activations_project = output_dict_project[layer_name2]\n",
    "        activation_batches_project.append(activations_project.to('cpu').detach().numpy().copy())\n",
    "\n",
    "    all_activations_relu = np.concatenate(activation_batches_relu, axis=0).flatten()\n",
    "    all_activations_project = np.concatenate(activation_batches_project, axis=0).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I5W9yY5DvOFr",
   "metadata": {
    "id": "I5W9yY5DvOFr"
   },
   "source": [
    "Thresholds calculated by MCT during quantization can be accessed using the following approach. The layer indices correspond to the order of the layers listed in the previous steps.\n",
    "\n",
    "As noted earlier, we focus on the first ReLU activation layer and the Batch Normalization layer (`expanded_conv_project_BN`) since they effectively illustrate the impact of the two threshold error methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NGnjrPD_uTd5",
   "metadata": {
    "id": "NGnjrPD_uTd5"
   },
   "outputs": [],
   "source": [
    "# layer 4 is the first activation layer - Conv1_relu\n",
    "optimal_thresholds_relu = {\n",
    "    error_method: data[\"quantized_model\"].features_0_2_activation_holder_quantizer.activation_holder_quantizer.threshold_np\n",
    "    for error_method, data in quantized_models_dict.items()\n",
    "}\n",
    "\n",
    "print(optimal_thresholds_relu)\n",
    "\n",
    "# layer 9 is the batch normalisation projection layer - Expanded_conv_project_BN\n",
    "optimal_thresholds_project = {\n",
    "    error_method: data[\"quantized_model\"].features_2_conv_2_bn_activation_holder_quantizer.activation_holder_quantizer.threshold_np\n",
    "    for error_method, data in quantized_models_dict.items()\n",
    "}\n",
    "\n",
    "print(optimal_thresholds_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XRAr8L5mvuLd",
   "metadata": {
    "id": "XRAr8L5mvuLd"
   },
   "source": [
    "### Distribution Plots\n",
    "Below are the activation distributions for the two selected layers: first, the ReLU activation layer, `Conv1_relu`, followed by the `expanded_conv_project_BN` layer.\n",
    "\n",
    "The second distribution clearly highlights the differences between the two error metrics, showing the impact of each on the resulting quantization threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VPb8tBNGpJjo",
   "metadata": {
    "id": "VPb8tBNGpJjo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ls_table = ['--', ':']\n",
    "lc_table = ['blue', 'red']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_activations_relu, bins=100, alpha=0.5, label='Original')\n",
    "for index, (method, threshold) in enumerate(optimal_thresholds_relu.items()):\n",
    "    plt.axvline(threshold, linestyle=ls_table[index], color=lc_table[index], linewidth=2, label=f'{method}: {threshold:.2f}')\n",
    "\n",
    "plt.title('Activation Distribution with Optimal Quantization Thresholds First Relu Layer')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Df7eKzh4oj5X",
   "metadata": {
    "id": "Df7eKzh4oj5X"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ls_table = ['--', ':']\n",
    "lc_table = ['blue', 'red']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_activations_project, bins=100, alpha=0.5, label='Original')\n",
    "for index, (method, threshold) in enumerate(optimal_thresholds_project.items()):\n",
    "    plt.axvline(threshold, linestyle=ls_table[index], color=lc_table[index], linewidth=2, label=f'{method}: {threshold:.2f}')\n",
    "\n",
    "plt.title('Activation Distribution with Optimal Quantization Thresholds Project BN layer')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c967d41-439d-405b-815f-be641f1768fe",
   "metadata": {
    "id": "4c967d41-439d-405b-815f-be641f1768fe"
   },
   "source": [
    "## Model Evaluation\n",
    "Finally, we can demonstrate the impact of these different thresholds on the model's overall accuracy.\n",
    "In order to evaluate our models, we first need to load the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631780a79e2cedf0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "val_dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy * 100.0))\n",
    "    \n",
    "    return test_loss, accuracy \n",
    "\n",
    "_, float_accuracy = evaluate(float_model, val_loader)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy * 100.0):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613",
   "metadata": {
    "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613"
   },
   "outputs": [],
   "source": [
    "evaluation_results = {}\n",
    "\n",
    "for error_method, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "\n",
    "    results = evaluate(quantized_model, val_loader)\n",
    "\n",
    "    evaluation_results[error_method] = results\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Results for {error_method}: Loss = {results[0]}, Accuracy = {results[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GpEZ2E1qzWl3",
   "metadata": {
    "id": "GpEZ2E1qzWl3"
   },
   "source": [
    "These results are consistent across many models, which is why MSE is set as the default method.\n",
    "\n",
    "Each of MCT's error methods impacts models differently, so it is recommended to include this metric as part of hyperparameter tuning when optimizing quantized model accuracy.\n",
    "\n",
    "\n",
    "## Conclusion\n",
    "In this tutorial, we explored the process of finding optimal activation thresholds using different error metrics in MCT’s post-training quantization workflow. By comparing the **MSE** and **No Clipping** methods, we demonstrated how the choice of threshold can significantly affect the activation distributions and, ultimately, the quantized model’s performance. While **MSE** is commonly the best choice and is used by default, it is essential to consider other error metrics during hyperparameter tuning to achieve the best results for different models.\n",
    "\n",
    "Understanding the impact of these thresholds on data loss and resolution is critical when fine-tuning the quantization process for deployment, making this a valuable step in building high-performance quantized models.\n",
    "\n",
    "\n",
    "## Appendix\n",
    "Below is a code snippet that can be used to extract information from each layer in the MCT quantization output, assisting in analyzing the layer-wise quantization details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qml4LLmWZLP4",
   "metadata": {
    "id": "qml4LLmWZLP4"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def flatten_model(modules):\n",
    "    def flatten_list(_2d_list):\n",
    "        flat_list = []\n",
    "        # Iterate through the outer list\n",
    "        for element in _2d_list:\n",
    "            if type(element) is list:\n",
    "                # If the element is of type list, iterate through the sublist\n",
    "                for item in element:\n",
    "                    flat_list.append(item)\n",
    "            else:\n",
    "                flat_list.append(element)\n",
    "        return flat_list\n",
    "\n",
    "    ret = []\n",
    "    try:\n",
    "        for _, n in modules:\n",
    "            ret.append(flatten_model(n))\n",
    "    except:\n",
    "        try:\n",
    "            if str(modules._modules.items()) == \"odict_items([])\":\n",
    "                ret.append(modules)\n",
    "            else:\n",
    "                for _, n in modules._modules.items():\n",
    "                    ret.append(flatten_model(n))\n",
    "        except:\n",
    "            ret.append(modules)\n",
    "    return flatten_list(ret)\n",
    "\n",
    "quantized_model = data[\"quantized_model\"]\n",
    "quantizer_object = quantized_model.features_0_2_activation_holder_quantizer\n",
    "\n",
    "quantized_model = data[\"quantized_model\"]\n",
    "\n",
    "relu_layer_indices = []\n",
    "\n",
    "target_layers =[]\n",
    "module_list =[module for module in quantized_model.modules()]\n",
    "flatted_list= flatten_model(module_list)\n",
    "\n",
    "for count, value in enumerate(flatted_list):\n",
    "    if isinstance(value, (nn.ReLU6)):\n",
    "        relu_layer_indices.append(count)\n",
    "\n",
    "print(\"Layer indices potentially using ReLU:\", relu_layer_indices)\n",
    "print(\"Number of relu layers \" + str(len(relu_layer_indices)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207",
   "metadata": {
    "id": "43f34133-8ed4-429a-a225-6fb6a6f5b207"
   },
   "outputs": [],
   "source": [
    "for error_method, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "    layer = quantized_model.features_0_0_bn\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "Copyright 2025 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ve310_1_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
