{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# Enhancing Post-Training Quantization with Z-Score Outlier Handling\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_activation_z_score_threshold.ipynb)\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates the process used to find the activation z-score threshold, a step that MCT can use during post-training quantization.\n",
    "\n",
    "In this example we will explore how setting different z scores effects threshold and accuracy. We will start by demonstrating how to apply the corresponding MCT configurations, then, we will feed a representative dataset through the model, plot the activation distribution of an activation layer with their respective MCT calculated z-score thresholds, and finally compare the quantized model accuracy of the examples of different z-score.\n",
    "\n",
    "## Managing Outliers with Activation Z-Score Thresholding\n",
    "During the quantization process, thresholds are used to map a distribution of 32-bit floating-point values to their quantized equivalents. Achieving this with minimal data loss while preserving the most representative range is crucial for maintaining the model’s final accuracy.\n",
    "\n",
    "Some models can exhibit anomalous values when evaluated on a representative dataset. These outliers can negatively impact the range selection, leading to suboptimal quantization. To ensure a more reliable range mapping, it is beneficial to remove these values.\n",
    "\n",
    "The **Model Compression Toolkit (MCT)** provides an option to filter out such outliers using **Z-score thresholding**, allowing users to exclude values based on their deviation from the standard distribution.\n",
    "\n",
    "The Z-score of a value is calculated by subtracting the dataset’s mean from the value and then dividing by the standard deviation. This metric indicates how many standard deviations a particular value is away from the mean.\n",
    "\n",
    "\n",
    "\n",
    "The threshold for outliers removal, $t$, is defined as a function of $Z_t$, the mean, $μ$, and the standard deviation, $σ$, of the activation values:\n",
    "\n",
    "$$\n",
    "t(Z_t) = μ + Z_t \\cdot σ\n",
    "$$\n",
    "\n",
    "\n",
    "Where:\n",
    "\n",
    "- $t(Z_t)$: The calculated threshold for outliers removal based on the Z-score threshold $Z_t$.\n",
    "- $Z_t$: The chosen Z-score threshold. It indicates how many standard deviations an activation value must be from the mean to qualify for removal or special handling prior to quantization.\n",
    "- $\\mu = \\frac{1}{n_s} \\sum_{X \\in F_l(D)} X$: The mean of activations\n",
    "- $\\sigma = \\sqrt{\\frac{1}{n_s} \\sum_{X \\in F_l(D)} (X - \\mu)^2}$: The standard deviation of activations in $F_l(D)$.\n",
    "    where:\n",
    "    - $F_l(D)$: Represents the distribution of activation values.\n",
    "    - $X$: An individual activation within the distribution.\n",
    "\n",
    "\n",
    "This equation for $t(Z_t)$ enables the identification of activation values that deviate significantly from the mean, helping to remove outliers before the main quantization step. This process results in a more reliable range for mapping floating-point values to quantized representations, ultimately improving quantization accuracy.\n",
    "## Setup\n",
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8e08612add2018",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ac404451fd924",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69daa2d5d731b157",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3dcd4f30be437e",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "### Download the ImageNet validation set\n",
    "Download the ImageNet dataset with only the validation split."
   ]
  },
  {
   "metadata": {
    "collapsed": false
   },
   "cell_type": "markdown",
   "source": [
    ":warning: **Note:** For demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ],
   "id": "a5584696d8f09653"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66a1e4f3878aa76b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bddd52741649281e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "root = Path('./imagenet')\n",
    "imgs_dir = root / 'ILSVRC2012_img_val'\n",
    "target_dir = root /'val'\n",
    "\n",
    "def extract_labels():\n",
    "    !pip install -q scipy\n",
    "    import scipy\n",
    "    mat = scipy.io.loadmat(root / 'ILSVRC2012_devkit_t12/data/meta.mat', squeeze_me=True)\n",
    "    cls_to_nid = {s[0]: s[1] for i, s in enumerate(mat['synsets']) if s[4] == 0} \n",
    "    with open(root / 'ILSVRC2012_devkit_t12/data/ILSVRC2012_validation_ground_truth.txt', 'r') as f:\n",
    "        return [cls_to_nid[int(cls)] for cls in f.readlines()]\n",
    "\n",
    "if not target_dir.exists():\n",
    "    labels = extract_labels()\n",
    "    for lbl in set(labels):\n",
    "        os.makedirs(target_dir / lbl)\n",
    "    \n",
    "    for img_file, lbl in zip(sorted(os.listdir(imgs_dir)), labels):\n",
    "        shutil.move(imgs_dir / img_file, target_dir / lbl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bc9fc",
   "metadata": {},
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1e7289b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36537e4308b48e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f40f3ea3fc8855",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "n_iter = 10\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7197e9b332c3bde",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Target Platform Capabilities\n",
    "MCT optimizes the model for dedicated hardware. This is done using TPC (for more details, please visit our [documentation](https://sony.github.io/model_optimization/api/api_docs/modules/target_platform.html)). Here, we use the default Tensorflow TPC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d179f65e3fc09f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a TargetPlatformCapabilities object that models the hardware platform for the quantized model inference. Here, for example, we use the default platform that is attached to a Pytorch layers representation.\n",
    "target_platform_cap = mct.get_target_platform_capabilities('pytorch', 'default')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
   },
   "source": [
    "## Post-Training Quantization using MCT\n",
    "This step we quantize the model with a few Z-score thresholds.\n",
    "The quantization parameters are predefined, and we use the default values except for the quantization method. Feel free to modify the code below to experiment with other Z-scores values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "jtiZzXmTjxuI",
   "metadata": {
    "id": "jtiZzXmTjxuI"
   },
   "outputs": [],
   "source": [
    "# List of error methods to iterate over\n",
    "q_configs_dict = {}\n",
    "\n",
    "# Z-score values to iterate over\n",
    "z_score_values = [3,5,9]\n",
    "\n",
    "# Iterate and build the QuantizationConfig objects\n",
    "for z_score in z_score_values:\n",
    "    q_config = mct.core.QuantizationConfig(\n",
    "        z_threshold=z_score,\n",
    "    )\n",
    "    q_configs_dict[z_score] = q_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8W3Dcn0jkJOH",
   "metadata": {
    "id": "8W3Dcn0jkJOH"
   },
   "source": [
    "Now we will run post-training quantization for each configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0c6e55-d474-4dc3-9a43-44b736635998",
   "metadata": {
    "id": "ba0c6e55-d474-4dc3-9a43-44b736635998"
   },
   "outputs": [],
   "source": [
    "quantized_models_dict = {}\n",
    "\n",
    "for z_score, q_config in q_configs_dict.items():\n",
    "    # Create a CoreConfig object with the current quantization configuration\n",
    "    ptq_config = mct.core.CoreConfig(quantization_config=q_config)\n",
    "\n",
    "    # Perform MCT post-training quantization\n",
    "    quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_dataset_gen,\n",
    "        core_config=ptq_config,\n",
    "        target_platform_capabilities=target_platform_cap\n",
    "    )\n",
    "\n",
    "    # Update the dictionary to include the quantized model\n",
    "    quantized_models_dict[z_score] = {\n",
    "        \"quantization_config\": q_config,\n",
    "        \"quantized_model\": quantized_model,\n",
    "        \"quantization_info\": quantization_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A8UHRsh2khM4",
   "metadata": {
    "id": "A8UHRsh2khM4"
   },
   "source": [
    "### Z-Score Threshold and Distribution Visualization\n",
    "To aid in understanding, we will plot the activation distribution of an activation layer in MobileNetV2. This distribution will be generated by inferring a representative dataset through the model.\n",
    "\n",
    "To visualize the activations, the model must be rebuilt up to and including the selected layer. Once the activations are extracted, we can calculate their Z-score threshold values manually using the equation provided in the introduction.\n",
    "\n",
    "Before plotting the distribution, we need to list the layer names. With PyTorch, this can be done easily using the following code. We determined the index of the layer of interest through a series of checks, which are detailed in the appendix section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print layer name\n",
    "layer_name = 'features.6.conv.2'\n",
    "print(layer_name)\n",
    "layer = dict(float_model.named_modules())[layer_name]\n",
    "print(layer)\n",
    "\n",
    "# add hook\n",
    "def get_layer_output(model, layer_name):\n",
    "    outputs = {}\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        outputs[layer_name] = output\n",
    "\n",
    "    # add hook\n",
    "    layer = dict(model.named_modules())[layer_name]\n",
    "    layer.register_forward_hook(hook)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "output_dict_relu = get_layer_output(float_model, layer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277",
   "metadata": {
    "id": "c38d28f3-c947-4c7c-aafa-e96cc3864277"
   },
   "source": [
    "The example activation layer in the model is named `conv_dw_8_relu`.\n",
    "\n",
    "We will use this layer name to build a model that ends at `conv_dw_8_relu`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364",
   "metadata": {
    "id": "ccc81508-01e5-421c-9b48-6ed3ce5b7364"
   },
   "source": [
    "Infer the representative dataset using these models and store the outputs for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb9888-5d67-4979-af50-80781a811b4b",
   "metadata": {
    "id": "eaeb9888-5d67-4979-af50-80781a811b4b"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "activation_batches_relu = []\n",
    "activation_batches_project = []\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    float_model = float_model.to(device)\n",
    "    for index, data in enumerate(tqdm(representative_dataset_gen())):\n",
    "        images = data[0]\n",
    "        images = images.to(device)\n",
    "\n",
    "        float_model(images)\n",
    "\n",
    "        activations_relu = output_dict_relu[layer_name]\n",
    "        activation_batches_relu.append(activations_relu.to('cpu').detach().numpy().copy())\n",
    "        \n",
    "    all_activations_relu = np.concatenate(activation_batches_relu, axis=0).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I5W9yY5DvOFr",
   "metadata": {
    "id": "I5W9yY5DvOFr"
   },
   "source": [
    "We can compute the Z-score for a layer using the formulas provided in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "WDx-LQSyxpDK",
   "metadata": {
    "id": "WDx-LQSyxpDK"
   },
   "outputs": [],
   "source": [
    "optimal_thresholds_relu = {}\n",
    "\n",
    "# Calculate the mean and standard deviation of the activation data\n",
    "mean = np.mean(all_activations_relu)\n",
    "std_dev = np.std(all_activations_relu)\n",
    "\n",
    "# Calculate and store the threshold for each Z-score\n",
    "for zscore in z_score_values:\n",
    "    optimal_threshold = zscore * std_dev + mean\n",
    "    optimal_thresholds_relu[f'z-score {zscore}'] = optimal_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XRAr8L5mvuLd",
   "metadata": {
    "id": "XRAr8L5mvuLd"
   },
   "source": [
    "### Distribution Plots\n",
    "In this section, we visualize the activation distribution from the constructed model along with the corresponding Z-score thresholds.\n",
    "From this list, we randomly select layers and evaluate their corresponding thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a1bef743d9711",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_error_thresholds = {\n",
    "    z_score: data[\"quantized_model\"].features_6_conv_0_2_activation_holder_quantizer.activation_holder_quantizer.threshold_np\n",
    "    for z_score, data in quantized_models_dict.items()\n",
    "}\n",
    "print(mse_error_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VPb8tBNGpJjo",
   "metadata": {
    "id": "VPb8tBNGpJjo"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "ls_table = ['--', ':', '-.', '--', ':']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(all_activations_relu, bins=100, alpha=0.5, label='Activations')\n",
    "for index, (z_score, threshold) in enumerate(optimal_thresholds_relu.items()):\n",
    "    random_color=np.random.rand(3,)\n",
    "    plt.axvline(threshold, linestyle=ls_table[index], linewidth=2, color=random_color, label=f'{z_score}, z-score threshold: {threshold:.2f}')\n",
    "    z_score_1 = int(z_score.split(' ')[1])  # Splits the string and converts the second element to an integer\n",
    "    error_value = mse_error_thresholds[z_score_1]  # Now using the correct integer key to access the value\n",
    "    plt.axvline(error_value, linestyle='-', linewidth=2, color=random_color, label=f'{z_score}, MSE error Threshold: {error_value:.2f}')\n",
    "\n",
    "plt.title('Activation Distribution with Optimal Quantization Thresholds - First ReLU Layer')\n",
    "plt.xlabel('Activation Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qbA6kFmw0vaf",
   "metadata": {
    "id": "qbA6kFmw0vaf"
   },
   "source": [
    "The impact of the Z-score on the error threshold is clearly visible here. A lower Z-score, such as 3, decreases the error threshold for the given layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c967d41-439d-405b-815f-be641f1768fe",
   "metadata": {
    "id": "4c967d41-439d-405b-815f-be641f1768fe"
   },
   "source": [
    "## Model Evaluation\n",
    "Finally, we can demonstrate how varying Z-score thresholds affect the model's accuracy.\n",
    "In order to evaluate our models, we first need to load the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edb94bd69d88e1a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf7d04-7816-465c-9157-6068c0a4a08a",
   "metadata": {
    "id": "8ebf7d04-7816-465c-9157-6068c0a4a08a"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, accuracy \n",
    "\n",
    "_, float_accuracy = evaluate(float_model, val_loader)\n",
    "print(f\"Float model's Top 1 accuracy on the Imagenet validation set: {(float_accuracy * 100.0):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613",
   "metadata": {
    "id": "07a22d28-56ff-46de-8ed0-1163c3b7a613"
   },
   "outputs": [],
   "source": [
    "#prepare quantised models and evaluate\n",
    "evaluation_results = {}\n",
    "\n",
    "for error_method, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "\n",
    "    results = evaluate(quantized_model, val_loader)\n",
    "\n",
    "    evaluation_results[error_method] = results\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Results for {error_method}: Loss = {results[0]}, Accuracy = {results[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GpEZ2E1qzWl3",
   "metadata": {
    "id": "GpEZ2E1qzWl3"
   },
   "source": [
    "We observe only minor improvements when adjusting the Z-score threshold. This pattern is common for most simple models. However, our testing shows that transformer models tend to benefit more from outlier removal. It is advisable to experiment with these parameters if the quantized accuracy is noticeably lower than the float model’s accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {
    "id": "14877777"
   },
   "source": [
    "## Conclusion\n",
    "In this tutorial, we demonstrated the use of Z-score thresholding as a critical step in the quantization process. This technique helps refine activation ranges by removing outliers, ultimately leading to improved quantized model accuracy. You can use the provided code as a starting point to experiment with selecting optimal Z-score thresholds for your own models.\n",
    "\n",
    "Our testing indicates that the optimal Z-score threshold typically falls between 8 and 12. Setting the threshold above 12 tends to show negligible improvement, while values below 8 may distort the distribution. However, finding the right threshold will require experimentation based on the specific characteristics of your model and use case.\n",
    "\n",
    "By applying Z-score thresholding thoughtfully, you can mitigate quantization errors and ensure that the quantized model's performance remains as close as possible to that of the original floating-point version."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BVHmePYJe7he",
   "metadata": {
    "id": "BVHmePYJe7he"
   },
   "source": [
    "## Appendix\n",
    "Below are selected code samples used to identify the most suitable layers for plotting thresholds and distributions.\n",
    "\n",
    "**Listing Layers Affected by Z-Score Adjustments**\n",
    "The following code snippet provides a list of layers that are impacted by Z-score thresholding, helping to determine which layers to focus on when visualizing distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cn-Ac9br9Ltz",
   "metadata": {
    "id": "cn-Ac9br9Ltz"
   },
   "outputs": [],
   "source": [
    "# Initialize a dictionary to hold threshold values for comparison\n",
    "thresholds_by_index = {}\n",
    "\n",
    "# Try to access each layer for each quantized model and collect threshold values\n",
    "for z_score, data in quantized_models_dict.items():\n",
    "    quantized_model = data[\"quantized_model\"]\n",
    "    module_list =[module for module in quantized_model.modules()]\n",
    "    for layer_index in range(1, len(module_list)):\n",
    "        try:\n",
    "            # Attempt to access the threshold value for this layer\n",
    "            threshold = module_list[layer_index].activation_holder_quantizer.threshold_np\n",
    "            # Store the threshold value for comparison\n",
    "            if layer_index not in thresholds_by_index:\n",
    "                thresholds_by_index[layer_index] = set()\n",
    "            thresholds_by_index[layer_index].add(threshold)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "# Find indices where threshold values are not consistent\n",
    "inconsistent_indices = [index for index, thresholds in thresholds_by_index.items() if len(thresholds) > 1]\n",
    "\n",
    "print(\"Inconsistent indices:\", inconsistent_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0YPqhQOh_N2r",
   "metadata": {
    "id": "0YPqhQOh_N2r"
   },
   "source": [
    "\n",
    "Next, we want to verify which layers correspond to the indices based on the layer names in the original float model. For example, index 52 has no matching layer, as it represents a quantized version of the previous layer. However, checking index 51 reveals that it aligns with the layer named `conv_dw_8_relu`, which we can use to plot the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rWGx5-6uu5H-",
   "metadata": {
    "id": "rWGx5-6uu5H-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "target_z_score = 9\n",
    "\n",
    "def get_leaf_layer_info(module, parent_name=''):\n",
    "    leaf_layer_info = []\n",
    "    \n",
    "    # Iterate through all child modules with their names\n",
    "    for name, child in module.named_children():\n",
    "        # Construct the full name of the child layer\n",
    "        full_name = f\"{parent_name}.{name}\" if parent_name else name\n",
    "        \n",
    "        # Recurse into the child module\n",
    "        leaf_layer_info.extend(get_leaf_layer_info(child, full_name))\n",
    "    \n",
    "    # If it has no children, it's a leaf layer; add its name and type\n",
    "    if not isinstance(module, nn.Sequential) and not isinstance(module, nn.ModuleList) and not list(module.children()):\n",
    "        leaf_layer_info.append((parent_name, type(module).__name__))\n",
    "\n",
    "    return leaf_layer_info\n",
    "\n",
    "quantized_model = quantized_models_dict[target_z_score][\"quantized_model\"]\n",
    "\n",
    "leaf_f_layer_info = get_leaf_layer_info(float_model)\n",
    "leaf_f_layer_info = [(n.replace('.', '_'), l) for n, l in leaf_f_layer_info]\n",
    "leaf_q_layer_info = get_leaf_layer_info(quantized_model)\n",
    "\n",
    "for index, (name, layer) in enumerate(leaf_f_layer_info):\n",
    "    search_string = str(name).replace('.', '_')\n",
    "\n",
    "    # Check if the target_z_score is in the quantized_models_dict\n",
    "    if target_z_score in quantized_models_dict:\n",
    "        data = quantized_models_dict[target_z_score]\n",
    "        # Iterate over each layer of the target quantized model\n",
    "        for quantized_index, (quantized_name, quantized_layer) in enumerate(leaf_q_layer_info):\n",
    "            found = search_string in str(quantized_name)\n",
    "            # If found, print details including the indices of the matching layers\n",
    "            if found:\n",
    "                print(f\"Float Model Layer Index {index} & Quantized Model Layer Index {quantized_index}: Found match in layer name  {search_string}, {quantized_name}\")\n",
    "    else:\n",
    "        print(f\"Z-Score {target_z_score} not found in quantized_models_dict.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AW_vC22Qw32E",
   "metadata": {
    "id": "AW_vC22Qw32E"
   },
   "outputs": [],
   "source": [
    "data[\"quantized_model\"].features_7_conv_1_0_bn.layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ve310_1_pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
