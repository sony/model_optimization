{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf96fb4",
   "metadata": {
    "id": "7cf96fb4"
   },
   "source": [
    "# Mixed-Precision Post-Training Quantization in PyTorch using the Model Compression Toolkit (MCT)\n",
    "[Run this tutorial in Google Colab](https://colab.research.google.com/github/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_mixed_precision_ptq.ipynb)\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a PyTorch model with post-training mixed-precision quantization. This quantization assigns different precision levels to various layers based on their impact on the model's output. We will load a pre-trained model and  quantize it using the MCT. Finally, we will evaluate the quantized model and export it to an ONNX file.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing ImageNetâ€™s validation dataset.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Applying mixed-precision post-training quantization to the model's weights using MCT.\n",
    "3. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0bb04",
   "metadata": {
    "id": "89e0bb04"
   },
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441efd2978cea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "if not importlib.util.find_spec('model_compression_toolkit'):\n",
    "    !pip install model_compression_toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82928d0",
   "metadata": {
    "id": "a82928d0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a pre-trained MobileNetV2 model from torchvision, in 32-bits floating-point precision format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c2556ce8144e1d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a302610146f1ec3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preparation\n",
    "### Download ImageNet validation set\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4df074784266e12e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir('imagenet'):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8a3327f28c20caf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ff2ea33659f0c1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = ImageNet(root='./imagenet', split='val', transform=weights.transforms())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18f57edc3b87cad3"
  },
  {
   "cell_type": "markdown",
   "id": "c0321aad",
   "metadata": {
    "id": "c0321aad"
   },
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the Post-Training Quantization (PTQ) algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618975be",
   "metadata": {
    "id": "618975be"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "n_iter = 10\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes models for dedicated hardware platforms using Target Platform Capabilities (TPC). \n",
    "**Note:**  To apply mixed-precision quantization to specific layers, the TPC must define different bit-width options for those layers. For more details, please refer to our [documentation](https://sony.github.io/model_optimization/docs/api/api_docs/modules/target_platform.html). In this example, we use the default PyTorch TPC, which supports 2, 4, and 8-bit options for convolution and linear layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caa87e1b976c9767"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a TargetPlatformCapabilities object that models the hardware platform for the quantized model inference. Here, for example, we use the default platform that is attached to a Pytorch layers representation.\n",
    "target_platform_cap = mct.get_target_platform_capabilities('pytorch', 'default')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "794b23bfe4a3f41d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mixed Precision Configurations\n",
    "We will create a `MixedPrecisionQuantizationConfig` that defines the search options for mixed-precision:\n",
    "1. **Number of images** - Determines how many images from the representative dataset are used to find an optimal bit-width configuration. More images result in higher accuracy but increase search time.\n",
    "2. **Gradient weighting** - Improves bit-width configuration accuracy at the cost of longer search time. This method will not be used in this example.\n",
    "\n",
    "MCT will determine a bit-width for each layer and quantize the model based on this configuration. The candidate bit-widths for quantization should be defined in the target platform model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad960242931c2d86"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "configuration = mct.core.CoreConfig(\n",
    "    mixed_precision_config=mct.core.MixedPrecisionQuantizationConfig(\n",
    "    num_of_images=32,\n",
    "    use_hessian_based_scores=False))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b26381aea3a94eb8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To enable mixed-precision quantization, we define the desired compression ratio. In this example, we will configure the model to compress the weights to **75% of the size of the 8-bit model's weights**. To achieve this, we will retrieve the model's resource utilization information, `resource_utilization_data`, specifically focusing on the weights' memory. Then, we will create a `ResourceUtilization` object to enforce the size constraint on the weight's memory, which applies only to the quantized layers and attributes (e.g., Conv2D kernels, but not biases)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f48885ac931bae5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get Resource Utilization information to constraint your model's memory size.\n",
    "resource_utilization_data = mct.core.pytorch_resource_utilization_data(\n",
    "    float_model,\n",
    "    representative_dataset_gen,\n",
    "    configuration,\n",
    "    target_platform_capabilities=target_platform_cap)\n",
    "\n",
    "weights_compression_ratio = 0.75  # About 0.75 of the model's weights memory size when quantized with 8 bits.\n",
    "# Create a ResourceUtilization object \n",
    "resource_utilization = mct.core.ResourceUtilization(resource_utilization_data.weights_memory * weights_compression_ratio)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "edee094198b3a558"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Post-Training Quantization with Mixed Precision\n",
    "Now, we are ready to use MCT to quantize the model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a64b5ce7a3f861e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "    in_module=float_model,\n",
    "    representative_data_gen=representative_dataset_gen,\n",
    "    target_resource_utilization=resource_utilization,\n",
    "    core_config=configuration,\n",
    "    target_platform_capabilities=target_platform_cap)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d769042646dca720"
  },
  {
   "cell_type": "markdown",
   "id": "c677bd61c3ab4649",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "In order to evaluate our models, we first need to load the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(dataset, batch_size=50, shuffle=False, num_workers=16, pin_memory=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57ee11ff6934aa9f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we will create a function for evaluating a model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ced59d1514509e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    \"\"\"\n",
    "    Evaluate a model using a test loader.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "            # correct += (predicted == labels).sum().item()\n",
    "    val_acc = (100 * correct / total)\n",
    "    print('Accuracy: %.2f%%' % val_acc)\n",
    "    return val_acc"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5f120e924b5d8cf4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's start with the floating-point model evaluation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a10499a2b79b19da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd038f7aff8cde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(float_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f564f31e253f5c",
   "metadata": {},
   "source": [
    "Finally, let's evaluate the quantized model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9da2134f0bde415",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(quantized_model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd09fa27",
   "metadata": {
    "id": "fd09fa27"
   },
   "source": [
    "Now, we can export the quantized model to ONNX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oXMn6bFjbQad",
   "metadata": {
    "id": "oXMn6bFjbQad"
   },
   "outputs": [],
   "source": [
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path='qmodel.onnx', repr_dataset=representative_dataset_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a classification model using the mixed precision feature with MCT. \n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights:\n",
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
